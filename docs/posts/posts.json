[
  {
    "path": "posts/2024-03-06-introducing-alarmdata/",
    "title": "Introducing `alarmdata`",
    "description": "The first stable version of our package `alarmdata` is now on CRAN, introducing\na data-focused package for using the outputs of ALARM Project research.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Michael Zhao",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2024-03-06",
    "categories": [],
    "contents": "\nalarmdata is the newest component of the redistverse, a family of packages which provide tools for redistricting analyses in R.\nAs of the past weekend, it is now on CRAN.\nYou can install the following with:\ninstall.packages('alarmdata')\nDownloading data\nalarmdata provides several functions for loading in data from ALARM Project projects.\nMost useful are alarm_50state_map() and alarm_50state_plans() which respectively load in the maps and plans from our 50-states project.\nThe map object contains Census and election for each state precinct.\nThe plans object contains alternative simulated plans.\nTo download and use these data, you can use the following code:\n\n\nlibrary(alarmdata)\nlibrary(redist)\n\nmap_wa <- alarm_50state_map('WA')\nplans_wa <- alarm_50state_plans('WA')\n\n\nUsing redist_plans with your own data\nOnce you’ve downloaded some plans, we’ve made it easy for you to add your own data to the redist framework.\nTo demo this, we can look at the NY plans passed last week (February 2024).\nFirst, we can load in the map and plans for New York:\n\n\nmap_ny <- alarm_50state_map('NY')\nplans_ny <- alarm_50state_plans('NY')\n\n\nNext, we can read in the block assignment data for the commission plan and the legislature plan.\nFor convenience of the example, we can grab this data from a GitHub repo.\n\n\n# commission link\ncomm_link <- 'https://raw.githubusercontent.com/christopherkenny/ny-baf/main/data/A09310.csv'\n\n# download the legislature xlsx file\nleg_link <- 'https://github.com/christopherkenny/ny-baf/raw/main/data-raw/congressional_plan_equivalency.xlsx'\ntemp_leg <- tempfile(fileext = '.xlsx')\ncurl::curl_download(leg_link, temp_leg)\n\n# read in the data\nbaf_commission <- readr::read_csv(comm_link, col_types = 'ci') |> \n  dplyr::rename(A09310 = district)\nbaf_legislature <- readxl::read_excel(temp_leg, col_types = c('text', 'text')) |> \n  dplyr::rename(\n    GEOID = Block,\n    commission2024 = `DistrictID:1`\n  ) |> \n  dplyr::mutate(commission2024 = as.integer(commission2024))\n\n\nFinally, alarmdata provides a function alarmdata::alarm_add_plan() to add the new reference plans to the underlying redist_plans object.\n\n\nplans_ny <- plans_ny |>\n  alarm_add_plan(ref_plan = baf_commission, map = map_ny, name = 'A09310') |>\n  alarm_add_plan(ref_plan = baf_legislature, map = map_ny, name = 'commission2024')\n\n\nThis gives a full redist_plans object with the new plans added in.\nWe can do anything we want with this.\nFor example, stealing a bit of code from our 50-states website, we can look at the Democratic vote shares across districts:\nFirst, we can define some helper functions for the plot:\n\n\nlbl_party <- function(x) {\n  dplyr::if_else(x == 0.5, \"Even\",\n    paste0(dplyr::if_else(x < 0.5, \"R+\", \"D+\"), scales::number(200 * abs(x - 0.5), 1))\n  )\n}\n\nr_geom <- function(...) {\n  ggplot2::geom_point(\n    ggplot2::aes(x = as.integer(.data$.distr_no),\n                 y = e_dvs,\n                 color = .data$draw, \n                 shape = .data$draw),\n    ...\n  )\n}\n\n\nThen, we can make a simple plot of Democratic vote share by district:\n\n\nlibrary(ggplot2)\nlibrary(ggredist)\nredist.plot.distr_qtys(plans_ny, e_dvs,\n                       color_thresh = 0.5,\n                       size = 0.04 - sqrt(8) / 250, alpha = 0.4,\n                       ref_geom = r_geom\n) +\n    geom_hline(yintercept = 0.5, color = '#00000055', size = 0.5) +\n    scale_y_continuous('Two-party vote margin', labels = lbl_party) +\n    labs(x = 'Simulated districts, ordered by Democratic vote margin') +\n    annotate('text',\n             x = 3.5, y = sort(plans_ny$e_dvs[1:26])[3],\n             label = 'Commission 2024', hjust = -0.05, size = 3.5,\n             color = '#52796F'\n    ) +\n    annotate('text',\n             x = 1.5, y = min(plans_ny$e_dvs[27:52]),\n             label = 'A09310', hjust = 0.05, size = 3.5,\n             color = '#A09310'\n    ) +\n    annotate('text',\n             x = 3.5, y = sort(plans_ny$e_dvs[53:78])[3],\n             label = 'Cervas', hjust = -0.05, size = 3.5,\n             color = 'black'\n    ) + \n\n    scale_color_manual(values = c('#52796F', '#A09310', 'black', ggredist$partisan[2], ggredist$partisan[14]),\n                       labels = c('pt', 'Rep.', 'Dem.'), guide = 'none') +\n    scale_shape_manual(values = c(16, 17, 18), guide = 'none') +\n    theme_bw()\n\n\n\nCache data across projects\nBy default, all downloads are directed to a temporary directory.\nEach time you reload R, you need to re-download the data.\nTo avoid this, you can set the alarm.use_cache option to a directory where you want to store the data.\noptions(alarm.use_cache = TRUE)\nThis can be set on a by-session basis, but we recommend setting it in your .Rprofile so that it is set every time you start R. To open this file, use the following command:\nusethis::edit_r_profile()\n\n\n\n",
    "preview": "posts/2024-03-06-introducing-alarmdata/introducing-alarmdata_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2024-03-06T13:30:06-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-10-26-louisvillejefferson-county-metro-government-balance-kentucky/",
    "title": "GUEST POST: Louisville/Jefferson County metro government (balance), Kentucky",
    "description": "An analysis of the electoral competitiveness of Louisville Metro Council Elections.",
    "author": [
      {
        "name": "Itsuki Umeyama",
        "url": {}
      }
    ],
    "date": "2023-12-13",
    "categories": [],
    "contents": "\n\n\nDISCLAIMER\n\nThis post is part of our ALARM Summer Research program, where social scientists in training study redistricting analysis to learn skills in computational social science. The results do not necessarily reflect findings of the broader ALARM team, and are not intended to be used as evidence or recommendations.\n\nCompetitiveness in Louisville Metro Council Elections\nHow competitive are the Metro Council elections in Louisville, Kentucky? While elections ideally should be competitive, assessing the competitiveness is challenging due to the absence of a baseline for comparison. To address this, I first generate simulated plans using the Sequential Monte Carlo (SMC) algorithm1 and then calculate the baseline using these plans. I found that the competitiveness of the Metro Council elections significantly decreased in 19 out of 26 districts. Furthermore, additional analysis reveals that the reduced competitiveness has favored the Democratic incumbent, resulting in a gain of 2 seats.\nAssessing Electoral Competitiveness with Algorithm-Assisted Redistricting Methodology\nApproach\nIdentifying the baseline of competitiveness is a challenging task, but the SMC algorithm provides a valuable tool for determining where the standard level of competitiveness in Louisville should lie. Originally developed to identify gerrymandering, this algorithm generates numerous plans with the same redistricting requirements, political geography, and physical geography as the current district plan. Consequently, it provides various baselines for assessing the district plan, including for the competitiveness of Metro Council elections.\nIn this assessment, electoral competitiveness is measured by the difference in vote counts between Democrats and Republicans. A smaller (larger) difference suggests higher (lower) competitiveness. I calculate the difference in vote counts of each district across all simulated plans, and then order the districts from the smallest difference in vote counts to the largest within the simulated plan. This process is repeated across 5,000 plans, generating distributions of differences in vote counts for each district, ordered accordingly. Finally, I assess the competitiveness of each current district by comparing it to the corresponding distributions.\nI consider a district has ideal competitiveness when its actual competitiveness falls within the 25th to 75th percentile of the corresponding distribution, or when it falls below the 25th percentile (more competitive than most of the simulated districts). Conversely, I consider the competitiveness of a district is notably reduced when the actual competitiveness exceeds the 75th percentile of the distribution.\nBasic Information on Louisville\nLouisville Metro Council\nThe Louisville Metro Council comprises 26 members who are elected from the city’s 26 districts in partisan elections. Council members serve staggered four-year terms. Since the merger with Jefferson County in 2003, the council has consistently maintained a Democratic majority. The distribution of seats held by each party is as follows (Democrats: Republicans): 15:11 (2006), 17:9 (2010), 17:9 (2014), 19:7 (2018), and 17:9 (2022)2.\nDemographic\nIn 2020, the population of Louisville stood at 386,884, with approximately 67% identifying as White, 24% as Black, 7% as Hispanic, and 3% as Asian3.\n\nFigure 1: This map shows the proportion of Black, Hispanic, and Asian residents within each precinct in Louisville. Darker shades of blue indicate a higher proportion of minorities, including Black, Hispanic, and Asian populations, living in the precinct compared to White population. The thinner black lines represent precinct boundaries, while the thicker white lines represent current district boundaries.Figure 2: This map shows precinct-level voting shares in Louisville (2022 Mayoral Election). A darker shade indicates a larger number of voters supporting either of the parties within the precinct. The black lines represent the current district boundaries, each marked with its corresponding district number.\nResults\nThe results suggest that there is considerable room for improvement in the competitiveness of Louisville’s Metro Council elections. Out of the 26 districts, 19 were found to be less competitive, while 4 districts are more competitive and 3 districts fall within the normal range of competitiveness (see the lower graph of Figure 3 for more details).\n\nFigure 3: The graph above shows the probability of winning between the Democratic Party and the Republican Party in each district, corresponding to the graph below. The graph below shows district competitiveness, measured by the difference in vote counts, with districts ordered by their level of competitiveness. A larger (smaller) difference in vote counts indicate the election is less (more) competitive. Both graphs are based on samples from the simulations.Figure 4: This map illustrates the results as follows: pink represents districts that are less competitive, blue represents districts that are more competitive, and yellow represents those falling within the normal range.\nDoes Reduced Competitiveness Benefit the Incumbent?\nGiven that the competitiveness of Metro Council elections is heavily influenced by the district plan in use, the significant decrease in competitiveness observed in Louisville raises a question: Does reduced competitiveness benefit the Democratic incumbent?\nIn addition to assessing competitiveness, I calculated the probability of each district being won by either Democrats or Republicans using simulated plans (as shown in the upper graph of Figure 3). This analysis helped me identify potential cases where decreased competitiveness creates a favorable situation for the incumbent. For example, if a district is less competitive and primarily Democratic, but the calculated probability suggests a likelihood of over 55% for a Republican victory, it raises the possibility of gerrymandering to reduce competitiveness.\nFigure 3 reveals that out of the 19 districts found to be less competitive, 4 of them (Districts 7, 17, 21, and 26) are currently represented by Democratic candidates, even though the probability indicates a greater likelihood of Republican victory with over 55%. However, on the other hand, this reduced competitiveness also favors the Republican Party in two districts (Districts 19 and 20).\nThis finding suggests that the reduced competitiveness in Louisville ultimately appears to contribute to creating favorable conditions for the incumbent, resulting in a 2-seat advantage.\nMethods and Materials\nMethods\nI generated 175,000 district plans in two independent runs of the SMC algorithm, with a constraint to minimize neighborhood splits. This constraint reflects Louisville residents’ concerns about the current district plan’s neglect of neighborhoods4. From these simulated plans, 5,000 were randomly selected for the analyses above.\nBefore generating simulated plans, I conducted the following pre-processing steps on the data:\nExcluded 32 precincts5 for which geographical data were unavailable.\nIncluded data from 5 elections held in 2022 (Mayor, US Senator, County Judge/Executive, County Clerk, and Coroner) to calculate the estimated baseline partisanship.\nNeighborhoods\nSince the merger with Jefferson County in 2003, there are three distinct types of perceived local boundaries in Louisville; Neighborhoods, Municipalities, and Places. All three boundary types hold importance in understanding the local communities in Louisville; therefore, I combined all geographical data of neighborhoods, municipalities, and places, into one dataset, eliminating any overlaps.\nNeighborhoods: lie within the Louisville city boundary that predates the city-county merger.\nMunicipalities: represent smaller cities within Louisville. Prior to the merger, these municipalities existed as independent governmental entities.\nPlaces: located outside the former Louisville city boundary, where people have historically settled densely. They are locally identified by their respective name.\nFigure 5: Neighborhoods used for redistricting simulations. Neighborhoods(yellow), Municipalities(green), and Places(blue).VRA Compliance\nSimulated plans must comply with the Voting Rights Act (VRA), and prevent gerrymandering that may disproportionately disadvantage minority populations. VRA compliance necessitates a specific number of minority-majority districts, where minorities constitute over 50% of the voting-age population. As shown in the figure below, Louisville already possesses approximately five Minority Opportunity-to-Elect districts. Therefore, I did not impose additional constraints in this regard.\nFigure 6: The graph shows minority voting age population share, with districts ordered from smaller share to larger share. Each points represents a simulated district, with blue indicating that the district is majority Democratic, and red indicating a majority Republican.Validation of the Simulated Plans\nIn addition to the VRA compliance, simulated plans should also be realistic enough to serve as viable alternatives to the enacted plan. They must incorporate redistricting requirements6, including population balance, geographical compactness, and the preservation of administrative boundaries. In the following graphs, we can see that the simulated plans are all unique, and they generally perform better without deviating significantly from the current plan in terms of population deviation, geographical compactness, and minority Voting Age Population (VAP) share. For neighborhoods, while simulated plans, on average, involve more splitting than the current plan, I attempted to minimize such splits by imposing a constraint.\nFigure 7: The validation plots above indicate that the simulated plans meet the basic criteria of redistricting. Firstly, these simulated plans are unique (with few similar plans), distribute populations equally in each districts, and have a sufficient level of compactness (see the first four graphs). Moreover, the graph in the third row, Compactness: Polsby-Popper, suggests that the current district plan could be made more compact. Additionally, the graph in the fifth row, Minority VAP share, indicates that in districts 3 and 4, minority groups are more concentrated than the simulated average, but aside from that, there is little evidence of arbitrary minority group clustering. The last three maps represent suggested plans from the simulations.Materials\nData\nElection Data\nGeneral elections in Louisville 2022\n\nGeographical Data\nPrecincts\nDistricts\nNeighborhoods, Municipalities, and Places\n\nConclusion\nThe competitiveness of Metro Council elections is heavily influenced by the district plan employed. However, assessing how closely the current competitiveness aligns with its potential level has been challenging due to the lack of a baseline for comparison. The SMC algorithm addresses this issue by generating numerous simulated plans with the same political geography and redistricting rules as the current one. I found a significant reduction in the competitiveness of Louisville Metro Council elections across 19 out of 26 districts. Furthermore, this decreased competitiveness favors the incumbent Democrat in the 2-seat advantage.\nIn addition to the assessment, I would like to highlight that considerable attention has been paid to respecting the neighborhoods of Louisville. Gauging how residents perceive their local communities is very difficult due to its subjective nature, but at the same time, ensuring that the district plan respects their neighborhoods stands out as one of the biggest concerns for residents. Louisville provides us with these challenging-to-measure psychological ties through three types of geographical information: Neighborhoods, Municipalities, and Places. I combined this spatial data into one dataset and used it when generating simulated plans to better align with the demands of Louisville residents.\n\nMcCartan, C., & Imai, K. (2023). Sequential Monte Carlo for sampling balanced and compact redistricting plans. The Annals of Applied Statistics, 17(4), 3300-3323.↩︎\nAll information regarding the elections in Louisville is based on the official statements of votes cast available at the website below. Jefferson County Clerk Election Center, Election Results, https://elections.jeffersoncountyclerk.org/election-results/↩︎\nUnited States Census Bureau, Quick Facts Louisville city, Kentucky; Louisville/Jefferson County metro government (balance), Kentucky, https://www.census.gov/quickfacts/fact/table/louisvillecitykentucky,louisvillejeffersoncountymetrogovernmentbalancekentucky/POP010220↩︎\nLouisville-Jefferson County Metro Government, Redistricting Information, https://louisvilleky.gov/government/metro-council/redistricting-information↩︎\nThe excluded precincts are: A141, A143, A155, B186, E186, E187, G197, H146, H161, H170, H172, H174, I143, J142, J143, J161, J167, K136, M187, M190, M204, M205, M206, M207, M208, M209, N124, N127, N159, O139, Q147, V135.↩︎\nRedistricting requirements in Louisville: in accordance with Kentucky state statutes, Metro Council boundaries must be compact and contiguous, have equal populations, and respect existing neighborhood, community, and city boundaries.↩︎\n",
    "preview": {},
    "last_modified": "2023-12-15T09:59:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-11-02-election-competitiveness-in-charlotte-north-carolina/",
    "title": "GUEST POST: Competitiveness in Charlotte, North Carolina",
    "description": "Analyzing competitiveness in Charlotte, North Carolina city council elections, as well as its influence on partisanship of each district.",
    "author": [
      {
        "name": "Angeline Zhao",
        "url": {}
      }
    ],
    "date": "2023-12-13",
    "categories": [],
    "contents": "\n\n\nDISCLAIMER\n\nThis post is part of our ALARM Summer Research program, where social scientists in training study redistricting analysis to learn skills in computational social science. The results do not necessarily reflect findings of the broader ALARM team, and are not intended to be used as evidence or recommendations.\n\nCompetitiveness in Charlotte City Council Elections\nCharlotte, North Carolina is a fascinating city where the levels of competition in mayor and city council elections have undergone significant changes in the last few years alone. By competition, in general, we mean how close the candidates in each election were in percentages. The city remains heavily Democratic from year to year. However, the level of competition candidates face each election cycle has changed significantly. Consider the mayoral elections, for example: in 2015, 52.26% voted Democrat and 47.60% voted Republican, whereas in 2022, 68.59% voted Democrat and 31.25% voted Republican. There is a clear decrease in competitiveness of mayoral elections during this time.\nHowever, taking a look at the city council election results shows that the partisan breakdown has not shifted once, with 5 Democratic districts and 2 Republican districts. Furthermore, the competitiveness in city council election varies greatly by district. The 5 Democratic districts have won with 95%, 82%, 77%, 95%, and 95% of the votes, whereas the 2 Republican districts have won with 51% and 89% of the votes. On average, the two Republican districts had much more competitive elections for city council.\nThe city council election results do not seem to fully align with the mayoral election results, as the former shows a lack of significant change in the partisan breakdown of the city whereas the latter indicates that there has been a notable change in favor of the Democratic party. As a result, this blog post will be exploring the reasons behind this by analyzing the expected competitiveness of city council elections and the expected partisan breakdown with simulations.\nThe overall results indicate that the competitiveness in the enacted city council map largely cancels out compared to the competitiveness in the simulations. However, the slight variation in competitiveness in specific city council districts highlights that one district is expected to be Democratic in the simulations but is Republican in the current map. This explains the lack of a shift in city council partisanship despite the significant overall shift in partisanship in Charlotte, as one of the current Republican city council districts should most likely be Democratic.\nOverview of Charlotte\nThe Charlotte City Council is made of the Mayor, four at-large city council members, and one more city council member for each of the seven city council districts. Currently, of the seven city council districts, five are Democratic and two are Republican.\nThe demographic breakdown in Charlotte is around 40% White, 33% Black, 16% Hispanic, and 7% Asian.\n\nFigure 1: The map above shows the percentage of minorities in each precinct, where yellow indicates smaller percentages and blue indicates larger percentages, with the city council districts outlined by the thick black lines. Grey areas had no recorded population.\nThe partisan breakdown in Charlotte is 69% Democrat and 31% Republican according to the latest 2022 mayoral election.\n\nFigure 2: The map above shows the partisan breakdown of Charlotte based on the latest mayoral election, where red indicates Republican and blue indicates Democratic, with the city council districts outlined by the thick black lines. Grey areas had no recorded population.\nAnalyzing Competitiveness and Partisanship In City Council Elections with Simulated Districts\nElection data from the 2022 mayoral election and city council at-large elections generated one set of redistricting simulation using the Sequential Monte Carlo (SMC) algorithm. For each of the simulations, a measure of competitiveness in each district is calculated as the absolute value of the difference in Democratic and Republican votes, divided by the total number of votes. This measure of competitiveness allowed for the comparison of which districts were more competitive without focusing on which party it favored yet. The measure of competitiveness described is always from zero to one, where lower values indicate more competition and higher values indicate less competition. I then calculated the competitiveness by district for simulated plans based on the 2022 mayoral election and city council at-large election to see if the currently enacted plan is more or less competitive compared to the simulations.\n\nFigure 3: The box-plot above shows the competitiveness in each city council district in order from most competitive to least competitive. The red lines indicate the enacted plan.\nConsidering the plot, we can see that there are four districts (1, 5, 6, 7) that are more competitive in the enacted plan and three districts (2, 3, 4) that are less competitive in the enacted plan than the simulations. The currently enacted plan is also on the border of or outside of the 25% to 75% range on the box-plot for all seven of these districts, so it is clearly an outlier compared to the simulations.\nNow we can consider the expected partisan breakdown in order to recognize the way the differing levels of competitiveness may have contributed to the city council partisan distribution. We can do this by considering the actual partisanship of each district versus the expected partisanship of each district across the simulations.\n\nFigure 6: The above boxplot shows the democratic vote share of simulation plans. The red line indicates the currently enacted plan.\nBy looking at this box plot, it is clear that six of these districts should be Democratic according to the simulations, but there are only five Democratic districts in the currently enacted plan. One of the districts that is currently Republican should be Democratic according to the simulations we ran.\nWe can recognize that the second ordered district is the one we are concerned about, as it is right on the edge between Democratic and Republican. More importantly, the plot shows us that in at least 75% of the simulations, the district is Democratic, which indicates that the enacted map is an anomaly.\nThese plots explain the difference between the partisan shift in the mayor election and in the city council election in Charlotte.\nMethods and Materials\nMethods\n30,000 redistricting plans for Charlotte, North Carolina were sampled over two independent runs of the SMC algorithm, then thinned the sample down to 5,000 plans.\nFor constraints, the neighborhood map was created with high school attendance zones in Charlotte. We use neighborhood maps when producing redistricting simulations as many cities attempt to preserve communities of interest when drawing new district maps in order to avoid splitting a single neighborhood into multiple city council districts. The high school attendance zones work well because communities of interest are very often linked by schools. Other potential neighborhood zones, like the Neighborhood Statistical Areas, historical districts, and Zip Codes were tested, but did not provide any benefit as a constraint because the neighborhood areas were not large enough or lacked coverage over the entire city. The neighborhood plot of school zones is shown below.\n\nWhen running simulations, we must also comply with the Voting Rights Act (VRA). VRA compliance attempts to prevent gerrymandering that disadvantages minority populations by requiring a certain number of minority-majority districts in which minorities can elect their candidate of choice. These districts are often created with a hinge constraint. These constraints work by creating a list of target percentages for each district and penalizing simulated maps based on adherence to those target percentages. Hinge constraints can then encourage the creation of more majority-minority districts for the purpose of VRA compliance. However, VRA compliance through the creation of these majority-minority districts is only required if voting patterns are polarized based on race. However, given that Charlotte is a very Democratic city on the whole, there is not a clear racial voting pattern that would indicate the need for the creation of more majority-minority districts. Most people vote Democrat no matter their race. As a result, we did not need a hinge constraint for VRA compliance. The minority VAP performance plot is shown below, which indicates the percentage of the minority VAP population by district, with red and blue indicating Republican and Democrat, respectively.\n\nHere are the other validation plots from running on the Charlotte 2022 election data. These validation plots are used to ensure that the simulations produced satisfy the conditions required for a redistricting map. The plan diversity map indicates that most plans are unique from one another, reducing the number of repeats throughout the simulations. The population deviation graph indicates the percent difference in population compared to a completely equal split of the population in each district. On this count, we do significantly better than the enacted plan. Compactness is important to redistricting maps, as districts that are not compact can signal gerrymandering. This is measured in two different ways, and both methods of calculating compactness show that the simulated plans tended to be more compact than the enacted plan. In terms of neighborhood splits, our simulations in fact perform significantly better. Similarly, the simulated plans do significantly better at reducing neighborhood splits compared to the enacted plan. Furthermore, the simulated plans show a similar number of majority-minority VAP districts. As a result, the Charlotte redistricting simulations perform very well in each of these important factors.\n\nData\nElection data came from the 2015 Mayoral election and the 2022 Mayoral and city council at-large elections. The North Carolina State Board of Elections organizes the election data by precinct, after which I kept only the candidates who were Democrats or Republicans in order to more clearly analyze the partisan competitiveness. One precinct with no voting data was removed, as well.\nThe Charlotte council district and precinct maps came from the City of Charlotte Open Data Portal. The precinct map of Charlotte was merged with the precinct election data, which was then filtered down to the council district boundaries.\nSources are linked below.\nElection Data\nDemographics\nCity of Charlotte GIS Map – Council Districts\nCity of Charlotte GIS Map – Precincts\nConclusion\nThe competitiveness of elections in Charlotte has heavily impacted the city council election results and districts. We can see that the level of competitiveness in each individual district in the currently enacted plan is an outlier in comparison to the simulated plans. By analyzing the results of these simulations, we have discovered that the current enacted plan benefits the Republican party as one seat that is expected to be a Democratic seat is instead a Republican seat. Out of just seven districts in Charlotte, one seat is fairly large difference.\nI focused mostly on competitiveness as well as how that impacted the partisan breakdown of the Charlotte city council for this blog post. In the future, I would like to look into analyzing the amount of harm these changes make for the individuals living in one district or another in order to better understand the actual impact certain plans have on the individuals in those areas.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-12-15T09:59:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-11-16-virginia-beach-city/",
    "title": "GUEST POST: Virginia Beach City",
    "description": "As one of many lawsuits against at-large voting systems for districts in local elections, Virginia Beach City recently adopted a new City Council district map that eliminates at-large seats. We explore the new implemented plan.",
    "author": [
      {
        "name": "Brian Zhou",
        "url": {}
      }
    ],
    "date": "2023-12-13",
    "categories": [],
    "contents": "\n\n\nDISCLAIMER\n\nThis post is part of our ALARM Summer Research program, where social scientists in training study redistricting analysis to learn skills in computational social science. The results do not necessarily reflect findings of the broader ALARM team, and are not intended to be used as evidence or recommendations.\n\n\nJump to analysis and key takeaways\n\nThe Virginia Beach City Council\nVirginia Beach became an incorporated town in 1906 from Princess Anne County, and became an independent city in 1952. A legal battle with the City of Norfolk prompted a consolidation of the City of Virginia Beach with Princess Anne County again in 1962, forming the independent city of Virginia Beach.\nVirginia Beach is heavily influenced by the presence of the military. Apart from the Pentagon in Arlington, Virginia, Virginia Beach has the largest concentration of military personnel with 9 military installations from every branch of the US armed forces; this includes the largest naval base in the world. Thus, it has been classified by the American Communities Project as a military post: “middle-income, diverse communities, around military bases.”\nThere are 11 members in the Virginia Beach City Council. Ten members of the city council are elected in one of ten districts as shown in Figure 1, including the current partisan affiliation of the council member. Displayed in Figure 2, three districts are currently Majority Minority Districts (MMDs), where the majority voting-eligible people in the district belong to a cohesive minority group.\n\n\nFigure 1: Current Partisan Breakdown of City Council Members\n\nFigure 2: Majority Minority Districts in Virginia Beach City\n\nThe eleventh member is elected at-large to serve as the mayor of the city. While the council is legally nonpartisan, candidates with political affiliations emerged just seven years after the formation of the city council in 1963. Historical elections have been competitive between both Democrats and Republicans with a few declared independents, but the overall majority of the city council historically leaned Republican, especially prior to the 2022 court-ordered redistricting. The current 2023 City Council is comprised of six Democrats and five Republicans (Figure 1), with Republican Bobby Dyer serving as mayor. No candidates were up for reelection in the November 7, 2023 Virginia general election.\nTable 1: Current Virginia Beach City Council makeup. Majority-Minority Districts are italicized.\nMayor\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nBobby Dyer (R)\nRocky Holcomb (R)\nBarbara M. Henley (D)\nMichael Berlucchi (R)\nAmelia Ross-Hammond (D)\nRosemary Wilson (R)\nWorth Remick (D)\nSabrina Wooten (D)\nChris Taylor (R)\nJoash Schulman (D)\nJennifer Rouse (D)\nThe ten districts are constructed from 108 voting precincts displayed in Figure 3 below. Census blocks, which are areas bounded by visible features such as roads or streams and invisible features such as property lines or other city/school districts, are visualized in Figure 4. These blocks help visualize where the city is located, as census blocks often follow city blocks in urban environments.\n\n\nFigure 3: Map of Voting Precincts and City Council Boundaries in Virginia Beach City\n\nFigure 4: Map of Census Blocks and City Council Boundaries in Virginia Beach City\n\nDemographics & Partisan Breakdown\nVirginia Beach City has a population of 457,672 recorded in the 2020 US census, making it the largest independent city in Virginia and the 42nd largest in the United States. It is ranked by the Census Bureau as the 36th largest urban area; its metropolitan statistical area (MSA), the Hampton Roads MSA, is the 37th largest metropolitan area in the US. The overall demographic breakdown of the city is in the table below; population data is sourced from the 2020 Census and VAP/CVAP data is sourced from a count by the Virginia Department of Elections in the 2019 Virginia elections.\nTable 2: Demographic Breakdown of Virginia Beach, VA. in 2020\nDemographic\nPopulation\nVoting Age Pop.\nCitizen Voting Age Pop.\nWhite, non-Hispanic\n58.7%\n61.5%\n65.8%\nBlack, non-Hispanic\n20.1%\n20.1%\n19.7%\nAsian, non-Hispanic\n10%\n9.4%\n6.8%\nHispanic\n8.8%\n7.6%\n6.4%\nBlack + Hispanic + Asian\n40.5%\n37.2%\n32.8%\nFigure 5 shows the population density of the overall city to a precinct level. This is important for drawing city council boundaries, as per the Virginia Voting Rights Act, a district can only deviate from the average population in every district by 5%. However, how do we know who is actually eligible to vote? Figure 6 shows the percentage of the population in any given precinct that is of voting age, which is more indicative of current voter demographics.\n\n\nFigure 5: Population Density by Precinct\n\nFigure 6: Percent Voting Age Population by Precinct\n\nFigures 7 and 8 show the percentage of minority and black populations by precinct. Districts 4, 7, and 10 are majority-minority districts (MMDs), shown in Figure 2.\n\n\nFigure 7: Minority Population Percentage by Precinct\n\nFigure 8: Black Population Percentage by Precinct\n\nIn Virginia Beach City, minority populations tend to vote as a block for Democratic candidates. Figure 9 displays the vote percentage of Democrats and Republicans in each precinct; Democrats perform best in black/minority precincts. To quantify how cohesive a group of voters are, Figure 10 examines only precincts where the majority of residents in the precinct is a minority. The percentage of democratic votes in any given precinct trends similarly to the overall percentage of minority residents in the precinct, ranging from a .78 to 1.05 ratio.\n\n\nFigure 9: Percent of Democratic Votes by Precinct. Color and percent indicate partisan tilt.\n\nFigure 10: Percent of Democratic Votes Compared to Minority Percentages in MMPs\n\nNotably, voters have different voting preferences compared to trends in federal elections. Although Figure 1 and Table 1 both show that there are 6 current Democratic council members, based on general voting trends, elections tend to tilt the following ways shown in Figure 11:\n\nFigure 11: Percent of Democratic Votes by District\nRedistricting in Virginia Beach\nHolloway v. City of Virginia Beach, Va.\nIn 2017, Virginia Beach residents Latasha Holloway and Georgia Allen sued the City of Virginia Beach claiming that Virginia Beach’s current at-large method to elect council members violated Section 2 of the Voting Rights Act by diluting the strength of minority votes in the legal case Holloway v. City of Virginia Beach, Va. Since 1998, Virginia Beach City Council members were elected in seven districts including three at-large council members; although candidates for one of the seven district seats had to reside in the district they were running for, all voting was conducted at-large. Residents of any district could vote in all seven district elections and the three at-large candidates.\nIn 2021, Virginia passed HB 2198, which prohibits local governments from using at-large voting for district elections. This led to the 2021 ruling in which the district court ruled that the at-large election method did violate Section 2 of the Voting Rights Act by denying large minority groups in Virginia Beach (African-Americans, Hispanics, and Asians) equal access to political participation, and that those groups were politically cohesive. Although the City of Virginia Beach appealed the decision, the court proceeded by appointing a special master to create a remedial redistricting plan, established in this memo. This plan was adopted in 2022 before the November election after another round of appeals in the Fourth Circuit. The Virginia Beach School Board, which historically followed the same election system as the city council, voted in 2022 to adopt the same election system as the city council.\nIn 2022, the Fourth Circuit Appeals Court ruled in a 2-1 decision that the case should not have been considered by the district court, holding that HB 2198 already made the electoral system challenged by the plaintiffs illegal. This meant that the at-large electoral system no longer governed Virginia Beach City already and rendered the court case moot. The timing of the ruling meant that the redrawn maps and electoral system would be used in 2022, and could be modified if it met the requirements of the Virginia Voting Rights Act and HB 2198, both of which prevent at-large voting systems if they prevent minority groups from electing preferred candidates.\nThe makeup of the city council drastically shifted in the 2022 general election, the first local election under the 10-1 system. The council is the most diverse and youngest in city history, with four Black representatives and four members under 45. In all of the city council’s history since 1966, only four prior city council members were Black, and at most had two seats on the city council at any point in time. Democrats gained two seats on the city council, shifting the makeup from 6 GOP and 4 DEM with a GOP mayor in 2022 to 6 DEM and 4 GOP in 2023.\nAfter a round of public hearings over the new election system and a survey that showed 81% of residents supported the new system, the city council voted to adopt the 10-1 election system with the court-sanctioned district map on August 15th, 2023. Per Virginia state law, any new electoral system, such as 7 districts and 4 at-large, would require approval from the Virginia General Assembly to revise the charter, which at earliest could happen when the General Assembly reconvenes in January 2024.\nAnalysis\nDo the New Maps Gerrymander Minority Populations?\nAs the new maps include 3 majority-minority districts compared to a minority CVAP of 32.8%, we find that the new implemented maps do not gerrymander minority populations and are compliant with both the Virginia Voting Rights Act and the Federal Voting Rights Act. Based on demographic shifts in younger voters (who have a higher minority population), a future map may necessitate the creation of a fourth majority-minority district.\nThe Virginia Voting Rights Act mandates that at-large local elections are banned if they dilute the voting power of minority populations. Instead, based on the percentage of the on the percentage of minorities in the citizen Voting Age Population and how cohesive minority voters are, minority opportunity districts would be created to protect minority voting power. Virginia Beach City has a cohesive democratic-leaning minority population that accounts for 32.8% of the citizen Voting Age Population. As there are 10 city council seats, three seats must be in MMDs.\nThe minority voting age population plot is displayed below. The graph orders each district from smaller to larger share of minority voting age population in each district. Each point represents one instance of a simulation, with the color of the point indicating if the district is projected to lean Democratic or Republican. The grey bars show the current ratified plan, showing that the current plan exceeds in creating three VAP districts. Notably, the enacted plan intentionally creates three solid majority-minority districts, demonstrated by the gap between the 7th and 8th districts.\nFigure 12: Simulated Majority Minority District CountsWe study the number of districts that are MMDs in any of the 200,000 plans. Total counts for performing MMDs in each simulation is displayed below, where a performing district has a minority VAP above 50% and an expected democratic vote share above 50%. This second criteria is less important, as every simulated district with an above 50% minority VAP is democratic.\nTable 3: Majority-Minority District appearance rate in 200,000 simulations\nCount of plans\n0 districts\n1 district\n2 districts\n3 districts\n4 districts\nPlans with n districts exactly\n3879\n54257\n116584\n24880\n400\nPlans with at least n districts\n200000\n196121\n141864\n25280\n400\n% plans with at least n districts\n100%\n98.06%\n70.93%\n12.64%\n2%\nTable 4: Majority-Black District appearance rate in 200,000 simulations\nCount of plans\n0 districts\n1 districts\nExactly n districts\n55007\n144993\n% of simulations with at least n districts\n100%\n72.50%\nAs simulations usually only create two MMDs (~116k simulations) or more (~71% occurence), it is clear that the enacted plan intentionally upholds the need for three MMDs based on the Virginia Voting Rights Act.\n\nFigure 13: Percent of All Simulations Where the Precinct is in a Simualted MMD.\nDo the New Maps Have Partisan Gerrymandering?\nAlthough there is evidence that two Democratic districts are being packed, Virginia Beach voters often vote split-ticket on the local level, having elected two Democratic council members in Republican-leaning seats.\nShown in Figure 14, many of the real districts appear to be packed compared to what the simulated districts would indicate.\nFigure 14: Democratic vote share by district in simulated runs. Black squares indicate the actual democratic vote share in the enacted plan, ordered by lowest to highest.We can get a better understanding with a box plot. The real Districts 9 and 10 appear to pack democratic voters compared to the simulated distribution of districts, while Districts 5 and 6 appear to be intentionally drawn to make the districts, whose median would be a toss-up and Democratic-leaning respectively, Republican.\nFigure 15: Boxplot of democratic vote share by district in simulated runs.One effective method of determining partisan gerrymandering is by measuring the efficiency gap of any given map, which is a value from -100% to 100% where negative implies pro-Democratic bias and positive implies pro-Republican bias. The current plan has an efficiency gap of 18.3% in favor of Republicans. Generally, an efficiency gap above 7% in favor of either party is regarded as partisan gerrymandering. This gives us evidence of partisan gerrymandering that favors Republicans in Virginia Beach City. We calculate the efficiency gap as the difference between votes from the party that loses a district and excess votes above 50% that weren’t necessary to win a district, divided by total votes.\nAre the New Maps Competitive?\nWhile five of the districts are competitive, the other five are extremely noncompetitive. In Figure 16, we look at the difference between the vote percentages of the winning and losing party in any given district. Defining a 10% difference as competitive, we can see that while four of the actual districts are competitive, the simulated plans could support all the way to eight competitive districts. At a minimum, two additional districts could easily be made competitive.\nFigure 16: Democratic vote share by district in simulated runs. Black squares indicate the actual democratic vote share in the enacted plan, ordered by lowest to highest.Instead of defining an arbitrary 10% difference, we can look at how competitive the actual districts are compared to the ensemble of simulated districts. Figure 17 shows that there are five competitive districts, and five extremely uncompetitive districts, four of which are outliers that indicate the plan was intentionally drawn to be uncompetitive. Any district that has good competitiveness would be any real district below the 25th percentile (a very small difference in vote count), indicating that it is more competitive than most simulated districts, and vice versa. Districts above the 75th percentile would have poor competitiveness.\nFigure 17: Boxplot of the absolute difference in vote counts.While this shows that the city council districts of Virginia Beach City could be made more competitive, it is likely that three of the five districts that are less competitive merely stem from creating three MMDs which lean Democratic in every simulation.\nMethods and Materials\nData\nWe gathered election data from the 2022 U.S. House election that took place in Virginia Beach City. Although the house district encompasses other parts of the Hampton Roads metropolitan area, the Virginia election results website includes breakdowns for all the precincts in Virginia Beach City. The U.S. House was selected, as only the 1st, 2nd, 4th, 6th, 8th, 9th, and 10th city council districts elected a member under the new system in 2022. This makes a full analysis of the city council election unfeasible, as prior elections for city council still operated under the at-large voting system.\nBecause the Virginia Department of Elections differentiates between early vote (by mail and in-person) and election day vote results for each precinct, data was sourced from two places in the Virginia election results website:\n2022 November General Election Virginia Beach City House of Representatives Votes by Precinct\n2022 November General Election Virginia Beach City House of Representatives Absentee Voting by Precinct\nWe compile the specific data (Election Day, Early Voting, Mailed Absentee, Provisional, and Post-Election) to create overall vote splits by precinct. 249 write-in ballots, which constituted 0.15% of the total vote in Virginia Beach, are discarded.\nRuns and Simulations\nWe sample 20,000 districting plans for Virginia Beach City across ten independent runs of the Sequential Monte Carlo algorithm according to the relevant criteria. A summary of simulations can be found below.\nRedistricting Requirements\nPer Virginia and Federal law under Va. Code Ann. § 24.2-304.04 redistricting in Virginia Beach must:\nhave equal populations\nprovide opportunities for racial/ethnic representation\npreserve communities of interest (Va. Code Ann. § 24.2-304.04(5).)\nbe contiguous (Va. Code Ann. § 24.2-304.04(6).)\nbe compact (Va. Code Ann. § 24.2-304.04(7).)\nIn the 2021 Virginia Redistricting, the following requirements were considered in order of priority:\nbe contiguous, compact, and equal in population\npreserve existing political subdivisions\ncreate clearly discernible boundaries\nbe reasonably compact\nmaintain neighborhoods who share similar social, cultural, and economic interests\nConstraints\nThree constraints are added to simulations to promote districts that will either have a cohesive black voting age population above 55% or a small black voting age population. This keeps historically black communities together by discouraging districts that have a percentage of black voting-age population between 20-40%.\nA constraint is created that strongly promotes a plan where one district has a black voting age population of at least 55%.\nA constraint is created that discourages where the proportion of the black voting age population is less than 40%.\nA constraint is created that encourages districts where the black voting age population is less than 20%.\nNeighborhoods\nWe utilize elementary school attendance zones to preserve similar communities of interest. School zoning is effective as neighborhoods are often shaped by the school district; school districts often create cohesion in other neighborhood-adjacent indicators such as community and economics, as it affects home prices and who chooses to reside in any given neighborhood. We utilize the 55 elementary school attendance zones to serve as a de-facto neighborhood map. School attendance zones usually correlate with historic areas of a city/county, and elementary school attendance zones are small enough to be beneficial for redistricting simulations while encompassing 2-3 neighborhoods.\n\n\nFigure 18: Map of Voting Precincts and City Council Boundaries in Virginia Beach City\n\nFigure 19: Map of Voting Precincts and Elementary School Boundaries in Virginia Beach City\n\nThese zones are necessary as neighborhoods are important for redistricting simulations; neighborhoods help make contiguous districts that account for community boundaries to avoid splitting a community between different districts. However, neighborhoods are difficult to find in Virginia Beach City, partially because the city fully incorporates Princess Anne County. Unlike other cities such as Phoenix, AZ, the city does not maintain any officially designated neighborhood or borough subdivision of the city. Various real estate agents maintain neighborhood designations, but they all have certain downsides:\nZiemer Real Estate’s neighborhoods are too large for use in preserving boundaries and have spots without neighborhoods\nLayton Realty Group’s neighborhoods are small and based on historic boundaries, but only cover the original area of Virginia Beach City\nCity-Data only has four tiny recorded neighborhoods for Virginia Beach City\nWe explore various other alternatives:\nZip codes were considered, but as there are only 20 zip codes in Virginia Beach City with odd shapes, using zip codes worsened the diversity (number of different simulated plans) in simulation outcomes\nCensus block maps were also considered, but are intentionally arbitrary to maintain similarly sized blocks across the United States and split neighborhoods\nUltimately, elementary school zoning serves as the most detailed indicator compared to sparse neighborhood data.\nSummary of Simulations\n\nFrom the validation plot of our simulations, we see that our simulation ranges around the same level of compactness as the enacted plan. Simulated plans are diverse, but push against the generally acceptable boundary of a maximum 5% population deviation between districts and vary greatly in neighborhood splits. One observation is that the enacted plan packs minority voters into three districts, which was motivated by a substantially lower citizen minority voter population across Virginia Beach City. In effect, minorities in the three districts lie around a 51-53% citizen VAP majority.\nTakeaways\nVirginia Beach City’s new implemented plan fulfills the required 3 MMDs set by the Virginia Voting Rights Act and the Federal Voting Rights Act based on it’s current Citizen Voting Age Population of 32.8%. Immediately the results are evident: the city council elected in the wake of the new map four black representatives, equivalent to the total number of black representatives since the council was formed in 1966 prior to the 2022 election.\nVoters also demonstrated willingness to look past party lines at the local level, voting in two Democratic city council members in districts that safely voted for the Republican representative in the same year.\nAlthough there is concern for partisan gerrymandering with a efficiency gap of 18.3% in favor of Republicans, it is possible that this is a side effect of creating all three MMDs; not a single MMD ever voted Republican in any of the 200,000 simulated plans, resulting in wasted Democratic votes that contributed to the high efficiency gap. Notably, voters often voted across party lines to deliver a 6-4 Democratic majority as opposed to the projected 6-4 Republican majority.\nInitial simulations also indicate that five districts are less competitive than comparable simulations. Currently, four of the districts have a gap of less than 10% in vote difference, but simulations reveal that future plans could create upwards of 8 competitive districts.\nAs more data using the new electoral system comes in the future, revisiting Virginia Beach City can help shed insights on how city voters adapted after its prior at-large voting system was struck down. Future work exploring further alternatives to elementary school zoning boundaries, particularly other neighborhood alternatives which may have more splits, would be helpful to more accurately simulate community interests.\n\n\n\n",
    "preview": "posts/2023-11-16-virginia-beach-city/FINALDemmmps.svg",
    "last_modified": "2023-12-15T09:59:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-10-15-commonwealth-opinion/",
    "title": "Using big data to make elections fairer",
    "description": "An opinion piece in CommonWealth Magazine with Ruth Greenwood.",
    "author": [
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\nIn a new opinion piece in CommonWealth Magazine this weekend, we discuss the work of ALARM and the Election Law Clinic. Our piece talks about the quantitative redistricting analysis projects, developed by the two groups, including the 50-State Redistricting Simulations, PlanScore, and RPV Near Me.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-10-18T08:59:01-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-14-census-bias-and-noise-wp/",
    "title": "Working Paper: Evaluating Bias and Noise Induced by the U.S. Census Bureau's Privacy Protection Methods",
    "description": "Our new working paper uses the new Noisy Measurement File release to understand bias and noise caused by swapping (1990-2010) and the TopDown algorithm (2020).",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-06-14",
    "categories": [],
    "contents": "\nWe are excited to announce a new working paper Evaluating Bias and Noise Induced by the U.S. Census Bureau’s Privacy Protection Methods. This paper is the first independent evaluation of effects of the Census Bureau’s privacy protection on noise and bias in released counts. We leverage the recent release of the Noisy Measurements file (NMF) to evaluate both swapping and the newer TopDown algorithm.\nWe find that:\nthe NMF is too noisy to use alone, but the post-processing step of the TopDown algorithm reduces error substantially, making the post-processed data as accurate as swapping.\nerrors from privacy protection methods are generally smaller than other sources of census errors, they can be substantial for census geographies with small populations.\nthe average bias is fairly low across groups, but there is more uncertainty and noise for Hispanic and multiracial people. Bias and RMSE estimates nationally for 5 geographic levels are displayed by race group below.\n\n\n\nThe full abstract is below:\n\nThe United States Census Bureau faces a difficult trade-off between the accuracy of Census statistics and the protection of individual information. We conduct the first independent evaluation of bias and noise induced by the Bureau’s two main disclosure avoidance systems: the TopDown algorithm employed for the 2020 nsus and the swapping algorithm implemented for the 1990, 2000, and 2010 Censuses. Our evaluation leverages the recent release of the Noisy Measure File (NMF) as well as the availability of two independent runs of the TopDown algorithm applied to the 10 decennial Census. We find that the NMF contains too much noise to be directly useful alone, especially for Hispanic and multiracial populations. TopDown’s post-processing dramatically reduces the NMF noise and produces similarly accurate data to swapping in terms of bias and noise. These patterns hold across census geographies with varying population sizes and racial diversity. While the estimated errors for both TopDown and swapping are generally no larger than other sources of Census error, they can be relatively substantial for geographies with small total populations.\n\n\n\n\n",
    "preview": "posts/2023-06-14-census-bias-and-noise-wp/bias_rmse_by_race.png",
    "last_modified": "2023-08-01T12:47:25-04:00",
    "input_file": {},
    "preview_width": 1950,
    "preview_height": 2700
  },
  {
    "path": "posts/2023-06-13-widespread-gerrymandering-pnas/",
    "title": "Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition Published in PNAS",
    "description": "Our paper which details gerrymandering and partisan fairness in the 2022 redistricting maps is now published in PNAS.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-06-13",
    "categories": [],
    "contents": "\nToday, our paper Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition was published in PNAS.\nWe evaluate partisan gerrymandering nationwide for new 2020 districts using simulated maps that account for geography and state-specific rules. We find evidence that gerrymandering is widespread across states, resulting in disadvantages for the Democratic party and less competitive districts. Read the abstract below:\n\nRedistricting plans in legislatures determine how voters’ preferences are translated into representative’s seats. Political parties may manipulate the redistricting process to gain additional seats and insulate incumbents from electoral competition, a process known as gerrymandering. But detecting gerrymandering is difficult without a representative set of alternative plans that comply with the same geographic and legal constraints. Harnessing recent algorithmic advances in sampling, we study such a collection of alternative redistricting plans that can serve as a non-partisan baseline. This methodological approach can distinguish electoral bias due to partisan effects from electoral bias due to other factors. We find that Democrats are structurally and geographically disadvantaged in House elections by 8 seats, while partisan gerrymandering disadvantages them by 2 seats.\n\nIf you’re interested in further details on this research project, take a look at the Supplementary Information or our replication data.\nThis research would not be possible without the 50-State Redistricting Simulations. A special thank you to George Garcia, Kevin Wang, and Melissa Wu.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-08-01T12:47:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-03-census-letter/",
    "title": "Letter: Researchers need better access to US Census data published in Science",
    "description": "Our letter providing recommendations to the Census Bureau about the Noisy Measurements File (NMF) now published in Science.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-06-03",
    "categories": [],
    "contents": "\nFor the 2020 decennial census, the Census Bureau adopted a new Disclosure Avoidance System (DAS) based on differential privacy.\nThe DAS was designed to protect the confidentiality of responses by injecting statistical noise into a confidential individual census dataset.\nA key output of this system is the Noisy Measurement File (NMF), which is produced by adding random noise to tabulated statistics.\nThe resulting Noisy Measurement File (NMF) is an invaluable resource for Census data users to understand the error introduced by the DAS and perform statistically valid analyses that properly account for DAS-introduced error.\nThe Bureau did not initially release the NMF, but released a demonstration version in April 2023 after several public requests and subsequent litigation. The Bureau plans to release the NMF for the P.L.94-171 redistricting data and more detailed census data (the DHC file) later this year.\nWe commend the Bureau’s decision to provide the NMF, which will help advance social science research, improve policy decisions, and further strengthen the DAS itself.\nTo maximize the benefits of the released NMF, however, we believe that the Bureau must substantially improve the way in which the NMF is formatted and released.\nIn a letter recently published in Science, we explain several obstacles researchers may face when accessing, processing, and using the demonstration data for statistical analyses. We include several recommendations for the Bureau for future NMF releases.\nOur longer version of this letter describes how researchers can transform the NMF into a usable format and includes more detailed recommendations.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-08-01T12:47:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-14-widespread-gerrymandering-in-pnas/",
    "title": "Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition Forthcoming in PNAS",
    "description": "Our paper describing partisan gerrymandering and competition in the 2022 US congressional districts is now forthcoming in PNAS.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2023-04-14",
    "categories": [],
    "contents": "\n\n\n\nWe are delighted to announce that “Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition” has been accepted for publication in PNAS, the Proceedings of the National Academy of Sciences of the United States of America. This article evaluates the partisan effects of redistricting in 2020 using simulations, allowing us to account for both compliance with legal requirements and the effects of political geography. While many individual state plans are biased in favor of one party or the other, the US House plan for 2022 is relatively unbiased. However, many districts are quite a bit less competitive than would be typically expected.\nA pre-print of the article is available on arXiv.\nWe are grateful to the editors and reviewers for their helpful feedback in improving this paper.\n\n\n\n",
    "preview": "posts/2023-04-14-widespread-gerrymandering-in-pnas/manipulation.png",
    "last_modified": "2023-04-14T20:20:46-04:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4350
  },
  {
    "path": "posts/2023-03-18-redist-41/",
    "title": "redist 4.1",
    "description": "A medium-sized release with more flexible plotting, better diagnostics, and\nspeed improvements.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      }
    ],
    "date": "2023-03-19",
    "categories": [],
    "contents": "\nIt’s been a while since redist 4.0 was released and things have been fairly stable. Most of the changes in this release are behind-the-scenes improvements that shouldn’t break your workflow, but should improve your experience using the package.\nTo install version 4.1, get the new version from CRAN:\n\n\ninstall.packages('redist')\n\n\nNew Features\nExtends the ordered box/jitter plots to custom ordered geometries in redist.plot.distr_qtys()\nBetter diagnostic outputs for summary.redist_plans()\nImproved confidence intervals with redist_ci()\nC++ improvements for sampling more quickly\nBetter sampling efficiency in SMC’s final stage\nQuicker random walks for SMC and merge-split.\nFaster random number generation. (It’s small, but it adds up!)\n\nPlotting Flexibility with redist.plot.distr_qtys()\nBox-and-whiskers plots are great and useful in many situations. In redistricting, we’ve often used ordered boxplots. These order the x-axis by the quantity on the x-axis. Sometimes, a boxplot throws away information that you might care about, though.\n>Is the distribution multi-modal? Where are the 2.5th and 97.5th percentiles for a confidence interval?\nNow, you can take those questions into your own hands with adjustments to arguments in redist.plot.distr_qtys()!\nLet’s build this out a bit. First, we’ll use some data from the 50-State Redistricting Simulations via the alarmdata package.\nWe can get the redist_map and corresponding 5,000 sampled plus the enacted plans in a redist_plans object for Michigan.\n\n\nlibrary(dplyr)\nlibrary(redist)\nlibrary(alarmdata)\nmap <- alarm_50state_map('MI')\nplans <- alarm_50state_plans('MI')\n\n\nplans here has a column e_dvs that gives the expected Democratic vote share for each district in each plan.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs)\n\n\n\nWe’ve always been able to clean or augment this plot up using regular ggplot2 things:\n\n\nlibrary(ggplot2)\n\nredist.plot.distr_qtys(plans, qty = e_dvs) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nYou can draw clear conclusions here, like that districts 2, 3, and 4 are abnormally packed with Republicans compared to what we might normally see when drawing districts that follow the state’s redistricting rules. We might not be able to get all of the information out of this plot that we want, beyond which districts are clear outliers.\nNow, if we made it a box plot instead of a jitter plot, as the points can be overwhelming, we can still draw the same major conclusions, and we now have a more formal idea of outliers that don’t just sit above or below the data. Things like district 6 can be still be a bit unclear. Is district 6 in the outlier range or are the points just big on this small plot?\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = 'boxplot') + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nA first thing we might consider doing is to use a violin plot instead of a boxplot, as that doesn’t summarize distributional information in the same way as a box plot. This is now really easy, just pass ggplot2::geom_violin as an argument to geom.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = ggplot2::geom_violin) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nThe defaults here don’t always play the best though, so we might want to also change the reference geometry.\n\n\nr_geom <- function(...) \n    ggplot2::geom_segment(\n        ggplot2::aes(x = as.integer(.distr_no) - 0.5,\n                     xend = as.integer(.distr_no) + 0.5,\n                     yend = e_dvs,\n                     color = .data$draw),\n        ...\n    )\n\n\nThis immediately gets a bit more complicated. For this to work, we need to know a few things:\nThe function has to take ... as an argument.\nInternally, the variable we are plotting on the x is going to be called .distr_no.\nThe reference geometry will inherit x = .distr_no by default and y = qty, for whatever your input to qty is.\nThe above then says, on the x-axis, we want a line from the district - 0.5 to the district + 0.5, while we set yend = e_dvs to match the implicitly set y = e_dvs, since we passed qty = e_dvs before.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = ggplot2::geom_violin, ref_geom = r_geom) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nThe good thing here is that we can adjust the ref_geom however we see fit at this point. So if that red line is too dark, but also too skinny, we can do something like changing the alpha:\n\n\nr_geom <- function(...) \n    ggplot2::geom_segment(\n        ggplot2::aes(x = as.integer(.distr_no) - 0.5,\n                     xend = as.integer(.distr_no) + 0.5,\n                     yend = e_dvs,\n                     color = .data$draw),\n        linewidth = 1, alpha = 0.7,\n        ...\n    )\n\n\nThen this fixes those particular issues.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = ggplot2::geom_violin, ref_geom = r_geom) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nNow, there are tons of other things we can do here. If we want to revisit the 95% confidence interval issue, we can turn to ggdist.\n\n\nlibrary(ggdist)\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = stat_pointinterval, ref_geom = r_geom) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.25, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nNow, we have really clear idea of how wide the 95% confidence interval goes (via the length of the skinny lines).\nAnd really, the sky is the limit with packages like ggdist. For example, if we want a raincloud, we can do that.\n\n\nraincloud <- function(...) {\n    list(\n        ggdist::stat_slab(aes(thickness = ggplot2::after_stat(pdf*n)), scale = 1),\n        ggdist::stat_dotsinterval(side = \"bottom\", scale = 1,\n                                  slab_size = NA, quantiles = 100)\n    )\n}\n\n\nThis gives us a fun plot to work with, though this might be best suited for much larger plot areas.\n\n\nredist.plot.distr_qtys(plans, qty = e_dvs, geom = raincloud, ref_geom = r_geom) + \n    geom_hline(yintercept = 0.5, linetype = 'dotted') + \n    scale_y_continuous(name = 'Expected Dem. Vote Share', labels = scales::label_percent(), \n                       limits = c(0.35, 0.95), breaks = seq(0.2, 0.9, by = 0.1)) +\n    theme_bw()\n\n\n\nBetter Diagnostics for summary.redist_plans()\nLike above, let’s get some simulated plans from the 50-State Redistricting Simulations. We can get a state like Nevada, which has fewer districts and shorter summary.\n\n\nlibrary(alarmdata)\nplans <- alarm_50state_plans('NV')\n\n\nTo get diagnostics, we can call summary(plans) which computes R-hats, sample diversity, and some split-by-split SMC diagnostics.\n\n\nsummary(plans)\n\n\n\nR-hat values for summary statistics:\n   pop_overlap      total_vap       plan_dev      comp_edge \n         1.003          1.003          1.001          1.000 \n   comp_polsby       pop_hisp      pop_white      pop_black \n         1.000          1.009          1.005          1.003 \n      pop_aian      pop_asian       pop_nhpi      pop_other \n         1.002          1.003          1.000          1.003 \n       pop_two       vap_hisp      vap_white      vap_black \n         1.003          1.007          1.006          1.004 \n      vap_aian      vap_asian       vap_nhpi      vap_other \n         1.001          1.002          1.000          1.007 \n       vap_two pre_16_dem_cli pre_16_rep_tru uss_16_dem_cor \n         1.003          1.001          1.003          1.000 \nuss_16_rep_hec uss_18_dem_ros uss_18_rep_hel gov_18_dem_sis \n         1.004          1.002          1.002          1.002 \ngov_18_rep_lax atg_18_dem_for atg_18_rep_dun sos_18_dem_ara \n         1.003          1.001          1.002          1.001 \nsos_18_rep_ceg pre_20_dem_bid pre_20_rep_tru         arv_16 \n         1.003          1.002          1.004          1.003 \n        adv_16         arv_18         adv_18         arv_20 \n         1.000          1.003          1.002          1.004 \n        adv_20  county_splits    muni_splits            ndv \n         1.002          1.002          1.003          1.002 \n           nrv        ndshare          e_dvs         pr_dem \n         1.003          1.003          1.003          1.001 \n         e_dem          pbias           egap \n         1.002          1.000          1.001 \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1     2,215 (88.6%)     19.7%        0.93 1,583 (100%)     10 \nSplit 2     2,287 (91.5%)     12.8%        0.51 1,565 ( 99%)      6 \nSplit 3     2,242 (89.7%)      5.9%        0.56 1,441 ( 91%)      4 \nResample    1,352 (54.1%)       NA%        0.57 1,409 ( 89%)     NA \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1     2,228 (89.1%)     15.1%        0.91 1,584 (100%)     13 \nSplit 2     2,285 (91.4%)      9.7%        0.52 1,574 (100%)      8 \nSplit 3     2,236 (89.4%)      5.0%        0.58 1,444 ( 91%)      5 \nResample    1,471 (58.8%)       NA%        0.59 1,399 ( 89%)     NA \n\nThe first big change here is that the digits are now rounded to three digits.\nYou no longer need to search through 8 decimal digits at 3am for the ones that matter.\nTypically, we want R-hat values between 1 and 1.05, so this looks pretty good.\nWhat if they weren’t? We can introduce this behavior by adding some new variable with very different values by independent run of SMC (denoted by the chain column).\n\n\nplans <- plans %>% \n    mutate(bad_rhat = rnorm(n = n(), mean = dplyr::coalesce(chain, 0)))\n\n\nNow this gets angry:\n\n\nsummary(plans)\n\n\n\nR-hat values for summary statistics:\n   pop_overlap      total_vap       plan_dev      comp_edge \n         1.003          1.003          1.001          1.000 \n   comp_polsby       pop_hisp      pop_white      pop_black \n         1.000          1.009          1.005          1.003 \n      pop_aian      pop_asian       pop_nhpi      pop_other \n         1.002          1.003          1.000          1.003 \n       pop_two       vap_hisp      vap_white      vap_black \n         1.003          1.007          1.006          1.004 \n      vap_aian      vap_asian       vap_nhpi      vap_other \n         1.001          1.002          1.000          1.007 \n       vap_two pre_16_dem_cli pre_16_rep_tru uss_16_dem_cor \n         1.003          1.001          1.003          1.000 \nuss_16_rep_hec uss_18_dem_ros uss_18_rep_hel gov_18_dem_sis \n         1.004          1.002          1.002          1.002 \ngov_18_rep_lax atg_18_dem_for atg_18_rep_dun sos_18_dem_ara \n         1.003          1.001          1.002          1.001 \nsos_18_rep_ceg pre_20_dem_bid pre_20_rep_tru         arv_16 \n         1.003          1.002          1.004          1.003 \n        adv_16         arv_18         adv_18         arv_20 \n         1.000          1.003          1.002          1.004 \n        adv_20  county_splits    muni_splits            ndv \n         1.002          1.002          1.003          1.002 \n           nrv        ndshare          e_dvs         pr_dem \n         1.003          1.003          1.003          1.001 \n         e_dem          pbias           egap       bad_rhat \n         1.002          1.000          1.001        ❌1.224 \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1     2,215 (88.6%)     19.7%        0.93 1,583 (100%)     10 \nSplit 2     2,287 (91.5%)     12.8%        0.51 1,565 ( 99%)      6 \nSplit 3     2,242 (89.7%)      5.9%        0.56 1,441 ( 91%)      4 \nResample    1,352 (54.1%)       NA%        0.57 1,409 ( 89%)     NA \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1     2,228 (89.1%)     15.1%        0.91 1,584 (100%)     13 \nSplit 2     2,285 (91.4%)      9.7%        0.52 1,574 (100%)      8 \nSplit 3     2,236 (89.4%)      5.0%        0.58 1,444 ( 91%)      5 \nResample    1,471 (58.8%)       NA%        0.59 1,399 ( 89%)     NA \n\nIt warns about convergence, as it has since 4.0. But it now also adds a big red “x” next to bad_rhat’s R-hat.\nAny questions? Open an issue on GitHub or find us on Twitter.\n\n\n\n",
    "preview": "posts/2023-03-18-redist-41/redist-41_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2023-05-01T15:48:15-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-01-31-comment-the-essenital-role-of-policy-evaluation/",
    "title": "Comment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System",
    "description": "Our response to boyd and Sarathy (2022) is now published in the HDSR!",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2023-01-31",
    "categories": [],
    "contents": "\nWe’re excited to share that Comment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System is now available at the Harvard Data Science Review. We discuss boyd and Sarathy (2022), addressing both factual inaccuracies in their work and their contention that disagreements over privacy in the 2020 Census are primarily academic issues.\n\nIn “Differential Perspectives: Epistemic Disconnects Surrounding the U.S. Census Bureau’s Use of Differential Privacy,” boyd and Sarathy argue that empirical evaluations of the Census Disclosure Avoidance System (DAS), including our published analysis (Kenny et al., 2021b), failed to recognize that the benchmark data against which the 2020 DAS was evaluated is never a ground truth of population counts. In this commentary, we explain why policy evaluation, which was the main goal of our analysis, is still meaningful without access to a perfect ground truth. We also point out that our evaluation leveraged features specific to the decennial census and redistricting data, such as block-level population invariance under swapping and voter file racial identification, better approximating a comparison with the ground truth. Lastly, we show that accurate statistical predictions of individual race based on the Bayesian Improved Surname Geocoding, while not a violation of differential privacy, substantially increases the disclosure risk of private information the Census Bureau sought to protect. We conclude by arguing that policymakers must confront a key trade-off between data utility and privacy protection, and an epistemic disconnect alone is insufficient to explain disagreements between policy choices.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-15-statistical-software-award/",
    "title": "redist Recieved POLMETH's 2022 Statistical Software Award",
    "description": "Our software won the Society for Political Methodology's Statistical Software Award.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Ben Fifield",
        "url": "https://www.benfifield.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2022-12-15",
    "categories": [],
    "contents": "\nLast week, Chris Kenny, Cory McCartan, Ben Fifield, and Kosuke Imai\nreceived this year’s Statistical Software Award from the Society for\nPolitical Methodology for the R package redist.\nThe announcement stated:\n\nThe redist package by Christopher T Kenny, Cory\nMcCartan, Ben Fifield, and Kosuke Imai of the Algorithm-Assisted\nRedistricting Methodology Project has rapidly become a key resource for\nresearchers and practitioners seeking to evaluate redistricting plans.\nredist develops statistically grounded and computationally\nefficient procedures for generating random draws from a distribution of\nviable redistricting plans, including conditional distributions that\nsatisfy specified requirements for geographic compactness and population\nparity. The package allows users to test for illegal partisan and racial\ngerrymandering, a timely and important question in the wake of the 2020\nCensus and the redistricting cycle that followed. It has had a\nsubstantial policy impact seeing use in legal challenges against and,\nunusually, has also been cited by six Supreme Court justices in oral\narguments. In short, it is an ideal recipient of the Society for\nPolitical Methodolgy’s 2022 Statistical Software Award.\n\nWe are grateful for the honor! Thank you to the award committee for\nconsidering our software.\nredist is an open source R package for sampling\nredistricting plans, available here:\n\nThis R package enables researchers to sample redistricting plans from\na pre-specified target distribution using Sequential Monte Carlo and\nMarkov Chain Monte Carlo algorithms. The package supports various\nconstraints in the redistricting process, such as geographic compactness\nand population parity requirements. Tools for analysis, including\ncomputation of various summary statistics and plotting functionality,\nare also included.\n\nredist is a key tool for much of our work, which has\nenabled work on identifying\nbias in Census 2020, creating\nalternative plans for all 50 states, reducing\nmalapportionment in Japan, assessing\npartisan bias in 2022’s congressional districts, and more.\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/alarm-redist/redist/main/man/figures/logo.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-01-50statessimulations/",
    "title": "50statesSimulations in Nature Scientific Data",
    "description": "Now published at Nature Scientific Data.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "George Garcia III",
        "url": {}
      },
      {
        "name": "Kevin Wang",
        "url": "https://scholar.harvard.edu/kevinwang"
      },
      {
        "name": "Melissa Wu",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2022-11-14",
    "categories": [],
    "contents": "\nWe’re excited to share that our article, Simulated redistricting\nplans for the analysis and evaluation of redistricting in the United\nStates, is now available through Nature\nScientific Data. This is a great step in making alternative\nredistricting plans more available for the public and researchers.\nCheck out the alarmdata R\npackage for some helpers for working with this data!\nWant more information? Take a look at the\nblog post that accompanied the release of the original working paper\nor an\nearly use of this data.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-13-policy-evaluation-commentary/",
    "title": "Comment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System",
    "description": "We're excited to announce our forthcoming article discussing boyd and Sarathy (2022).",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2022-10-18",
    "categories": [],
    "contents": "\nWe’re excited to announce a new paper, Comment: The Essential Role of\nPolicy Evaluation for the 2020 Census Disclosure Avoidance System,\nnow forthcoming at the Harvard Data Science Review. This\narticle responds to boyd and\nSarathy (2022). This builds on our research into\nthe impact of differential privacy on redistricting and advances our\nresponses\nto some common questions.\nThe abstract is listed below:\n\nIn “Differential Perspectives: Epistemic Disconnects Surrounding the\nUS Census Bureau’s Use of Differential Privacy,” boyd and Sarathy argue\nthat empirical evaluations of the Census Disclosure Avoidance System\n(DAS), including our published analysis, failed to recognize how the\nbenchmark data against which the 2020 DAS was evaluated is never a\nground truth of population counts. In this commentary, we explain why\npolicy evaluation, which was the main goal of our analysis, is still\nmeaningful without access to a perfect ground truth. We also point out\nthat our evaluation leveraged features specific to the decennial Census\nand redistricting data, such as block-level population invariance under\nswapping and voter file racial identification, better approximating a\ncomparison with the ground truth. Lastly, we show that accurate\nstatistical predictions of individual race based on the Bayesian\nImproved Surname Geocoding, while not a violation of differential\nprivacy, substantially increases the disclosure risk of private\ninformation the Census Bureau sought to protect. We conclude by arguing\nthat policy makers must confront a key trade-off between data utility\nand privacy protection, and an epistemic disconnect alone is\ninsufficient to explain disagreements between policy choices.\n\nFor those interested in the full commentary, a preprint is available\nhere.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-30-nikkei/",
    "title": "'One vote disparity' can be improved with state-of-the-art algorithms",
    "description": "Our article in Nikkei Business on reducing Japanese malapportionment was released!",
    "author": [
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      },
      {
        "name": "Kento Yamada",
        "url": {}
      },
      {
        "name": "Rei Yatsuhashi",
        "url": {}
      },
      {
        "name": "Sho Miyazaki",
        "url": {}
      }
    ],
    "date": "2022-09-30",
    "categories": [],
    "contents": "\nALARMプロジェクトは、日経ビジネスの依頼を受け、衆議院の区割り改定案の作成とその分析の記事を寄稿しました。区割りシミュレーションアルゴリズムの活用により、区画審の改定案よりも、市区町村の分割数と一票の格差が少なくなる区割り案が作成できます。https://business.nikkei.com/atcl/gen/19/00351/092100048/\nWe were invited to contribute to Nikkei Business (Japan)! We show\nthat the SMC redistricting simulation algorithm can be used to reduce\nmalapportionment without splitting more municipalities, when compared to\nthe districting plan proposed by the commission. https://business.nikkei.com/atcl/gen/19/00351/092100048/\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-14-gerrymandering-mostly-cancels-nationally/",
    "title": "Widespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition",
    "description": "Gerrymandering in 2020 redistricting makes the US House elections less \ncompetitive, but net seat gains are small nationally. The partisan bias of the\nenacted national map is about as biased as non-partisan simulations, due to\ngeography and legal requirements.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2022-08-16",
    "categories": [],
    "contents": "\nWe’re excited to release a\nnew working paper studying partisan bias in the 2020 US House plan.\nWe employ redistricting simulations from the 50stateSimulations.1 with a model of partisanship to dig\ninto geographic details of what happened where. Gerrymandering in\n2020 redistricting makes the US House elections less competitive, but\nnet seat gains are small nationally. The abstract below highlights\nmore of our findings.\n\nCongressional district lines in many U.S. states are drawn by\npartisan actors, raising concerns about gerrymandering. To isolate the\nelectoral impact of gerrymandering from the effects of other factors\nincluding geography and redistricting rules, we compare predicted\nelection outcomes under the enacted plan with those under a large sample\nof non-partisan, simulated alternative plans for all states. We find\nthat partisan gerrymandering is widespread in the 2020 redistricting\ncycle, but most of the bias it creates cancels at the national level,\ngiving Republicans two additional seats, on average. In contrast,\nmoderate pro-Republican bias due to geography and redistricting rules\nremains. Finally, we find that partisan gerrymandering reduces electoral\ncompetition and makes the House’s partisan composition less responsive\nto shifts in the national vote.\n\nAll in all, most of the states you thought were gerrymandered are\nindeed gerrymandered.\nSome gerrymandered states that you might have missed include those\ndrawn by commissions (like Michigan and Iowa) and those drawn by courts\n(like Pennsylvania and North Carolina). The state-by-state results are\nbelow, with a national topline that the US map favors Republicans by\nabout two seats beyond what’s explained by geography, on average.\n\n\n\nWe can take this further and look at a partisan manipulation\nmap. Each district is colored by the difference in probability that\nit is represented by a Republican or Democrat in the enacted from the\nsimulated plans. Red areas favor Republicans over simulations, blue\nareas favor Democrats over simulations. The darkness of each district\nrepresents the intensity of that difference.\n\n\n\nIf you’re interested in more information on our findings or methods,\ntake a look here.\nA special thank you to George Garcia III, Kevin Wang, and Melissa Wu\nfor their contributions to the 50stateSimulations which\nmade this research possible.\n\nIn case you missed it, we\nhave a blog post introducing those simulations and their\ncontributors.↩︎\n",
    "preview": "posts/2022-08-14-gerrymandering-mostly-cancels-nationally/state_sum_1.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 3300
  },
  {
    "path": "posts/2022-06-23-fifty-states-data-descriptor/",
    "title": "Fifty States Data Descriptor",
    "description": "A detailed description of the 50-State Redistricting Simulations \nand new software to help you use them.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      },
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Tyler Simko",
        "url": "https://tylersimko.com/"
      },
      {
        "name": "George Garcia III",
        "url": {}
      },
      {
        "name": "Kevin Wang",
        "url": "https://scholar.harvard.edu/kevinwang"
      },
      {
        "name": "Melissa Wu",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": "https://www.shirokuriwaki.com/"
      },
      {
        "name": "Kosuke Imai",
        "url": "https://imai.fas.harvard.edu/"
      }
    ],
    "date": "2022-07-28",
    "categories": [],
    "contents": "\nIt’s been a long redistricting year. We’ve been\ntracking passed maps while conducting simulations in the 44 states with\ncongressional districts. We are now finalizing some re-runs of states\nwith new validation steps based on additional diagnostics, to ensure a\nhigh quality and accurate data product. So, we’ve written up a more\ndetailed draft of what we did, how we did it, and how we checked our\nwork. Most importantly, it introduces some tools so that you can use the\ndata we’ve generated. It’s all open source and the redistricting plans\ngenerated are in the public domain.\nRead the detailed description of the our process and the data: Simulated redistricting plans\nfor the analysis and evaluation of redistricting in the United States:\n50stateSimulations. The abstract is listed below.\n\nThis article introduces the 50stateSimulations, a collection of\nsimulated congressional districting plans and underlying code developed\nby the Algorithm-Assisted Redistricting Methodology (ALARM) Project. The\n50stateSimulations allow for the evaluation of enacted and other\ncongressional redistricting plans in the United States. While the use of\nredistricting simulation algorithms has become standard in academic\nresearch and court cases, any simulation analysis requires non-trivial\nefforts to combine multiple data sets, identify state-specific\nredistricting criteria, implement complex simulation algorithms, and\nsummarize and visualize simulation outputs. We have developed a complete\nworkflow that facilitates this entire process of simulation-based\nredistricting analysis for the congressional districts of all 50 states.\nThe resulting 50stateSimulations include ensembles of simulated 2020\ncongressional redistricting plans and necessary replication data. We\nalso provide the underlying code, which serves as a template for\ncustomized analyses. All data and code are free and publicly available.\nThis article details the design, creation, and validation of the\ndata.\n\nTo help make things more usable for those who don’t simulate\nredistricting plans in their free time, we are also excited to (soft)\nlaunch a new R package, alarmdata.\nThis package provides a simplified interface to download the underlying\ngeographic data, generated plans, all sorts of summary statistics, and\nstate-by-state documentation.\nThe package can be installed with:\n\n\nremotes::install_github('alarm-redist/alarmdata')\n\n\nThank you to the Harvard Data Science Initiative and Microsoft for\ncomputational support.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-06-japanese-society-of-quantitative-political-science/",
    "title": "47-Prefectures at the Japanese Society of Quantitative Political Science",
    "description": "We are presenting Friday on malapportionment. 私たちは、アルゴリズムを用いた一票の格差の是正について、金曜日に発表します。",
    "author": [
      {
        "name": "Sho Miyazaki",
        "url": {}
      },
      {
        "name": "Kento Yamada",
        "url": {}
      },
      {
        "name": "Rei Yatsuhashi",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2022-07-07",
    "categories": [],
    "contents": "\nWe are presenting a poster presentation at the Japanese Society of\nQuantitative Political Science 🇯🇵! We show that the well-known\nmalapportionment (一票の格差) of the Japanese House of Representatives\ncan be redressed without splitting administrative boundaries.\nALARMプロジェクトは、計量・数理政治研究会（JSQPS）の夏季集会にてポスターセッションに参加致します。アルゴリズムを衆議院の選挙区改変に応用して、市区町村の分割数を増やさなくても、一票の格差が是正される区割り案を作成できることを発表します。\nView\nthe poster! Or\nsee the full details of the project.\nポスターをご覧ください！\n詳細は、本プロジェクトのウェブサイトをご確認ください。\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-20-redist-40/",
    "title": "redist 4.0",
    "description": "A major release with big changes to constraints and diagnostics.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": "https://www.christophertkenny.com/"
      },
      {
        "name": "Cory McCartan",
        "url": "https://www.corymccartan.com"
      }
    ],
    "date": "2022-06-20",
    "categories": [],
    "contents": "\nWe are excited to announce the arrival of redist 4.0.1\non CRAN. This\nupdate focuses on increasing constraint consistency and diagnostic\nusability. The new tools here have been thoroughly tested as part of the\n50-State\nRedistricting Simulations project.\nTo install the new version, run\ninstall.packages('redist').\nNew Features\nA new constraint interface that is more flexible, user friendly, and\nconsistent across algorithms (see redist_constr() and\n?constraints). For the first time, user-defined custom\nconstraints are supported and integrated within all three\nalgorithms.\nNew diagnostic-checking function,\nsummary.redist_plans()\nSummary statistics have been broken out into a new\nredistmetrics package This will speed up compilation time\nand also provides a cleaner, more extensible interface for the\nimplementation of additional metrics.\nParallel computing support for the SMC algorithm, both within and\nacross sampling runs\nReproducible across-run parallelism throughout the package, via\ndoRNG\nMuch faster match_numbers() using the Hungarian\nmethod\nmin_move_parity() calculates how much population needs\nto be moved between districts in order to completely balance a\nredistricting plan.\nSupport for partial SMC simulations, where fewer districts are drawn\nthan the total number. Allows advanced users to manually combine partial\nruns to form complete maps.\nImproved algorithm reporting, including new progress bars and\ncli errors and warnings throughout the package\nUpdate the SMC algorithm to include a missing correction factor for\nthe number of ways to sequentially label districts. This factor should\nnot have an effect on substantive conclusions and summary\nstatistics.\nRemove deprecated functions\nMany bug fixes (see https://github.com/alarm-redist/redist/issues)\nUpdated Features: A Brief\nDemo\nThe first thing you’ll notice upon loading redist is\nthat it also loads redistmetrics.\nredistmetrics used to live within redist but\nhas been separated to keep the package size reasonable and to make the\nindividual compile times shorter.\n\n\nlibrary(tidyverse)\nlibrary(redist)\n\n\n\nWe can pull in some\ndata from the ALARM Project, which combines 2020 Census data with VEST’s\nelection data, retabulated to 2020 voting districts. For this example,\nwe can use data from New Mexico.\n\n\nnm <- geomander::get_alarm('NM')\n\n\n\n\nWe can then make a redist_map for New Mexico.1\n\n\nmap_nm <- redist_map(nm, ndists = 3, pop_tol = 0.005)\n\n\n\nAnd we can begin with a basic run of redist_smc to\nsample 1000 plans using the sampler from Sequential Monte Carlo for\nSampling Balanced and Compact Redistricting Plans by Cory McCartan and Kosuke Imai. Most importantly,\nredist_smc now offers an argument for the number of\nindependent sampling runs. For now, we can break that 1000 plans into 2\nruns of 500.\n\n\nset.seed(2022)\nplans <- redist_smc(map = map_nm, nsims = 500, runs = 2, counties = county)\n\n\n\nThe new messages above are created with cli to more make\nmessage printing cleaner and more consistent.\nTo the output, we can add some basic summary information using\navailable redistmetrics functions, automatically loaded by\nredist.\n\n\nplans <- plans %>% \n    mutate(\n        frac_kept = comp_frac_kept(plans = ., shp = map_nm),\n        dvs_gov_18 = part_dvs(plans = ., shp = map_nm, dvote = gov_18_dem_luj, rvote = gov_18_rep_pea),\n        county_spl = splits_admin(plans = ., shp = map_nm, admin = county)\n    )\n\n\n\nIn order, this adds the Fraction Kept compactness score, the\nDemocratic two-party vote share in the 2018 Governor’s race, and the\nnumber of counties split.\nNow, the plans object has a few new columns:\n\n\nhead(plans)\n\n\nWith plans resampled from weights\nPlans matrix: int [1:1977, 1:1000] 3 3 3 3 3 3 3 3 3 3 ...\n# A tibble: 6 × 7\n  draw  chain district total_pop frac_kept dvs_gov_18 county_spl\n  <fct> <int>    <int>     <dbl>     <dbl>      <dbl>      <int>\n1 1         1        1    706220     0.984      0.474          2\n2 1         1        2    707900     0.984      0.602          2\n3 1         1        3    703402     0.984      0.619          2\n4 2         1        1    707157     0.990      0.508          2\n5 2         1        2    706801     0.990      0.582          2\n6 2         1        3    703564     0.990      0.616          2\n\nDraw, chain, and district identify each plan, where\nchain is new to 4.0 for SMC. It signifies the SMC run,\nsimilar to how redist_mergesplit_parallel indicates the\nchain from merge-split. Despite this, we can use the normal plotting\nfunctions on the redist_plans object. If we load\npatchwork here to get a nice row of ggplots,\nwe see the following:\n\n\nlibrary(patchwork)\nhist(plans, frac_kept) + \n    plot(plans, dvs_gov_18) + \n    hist(plans, county_spl)\n\n\n\n\nThese plots are fairly standard. The exciting thing is that we can\nnow call summary() to get diagnostic information about the\nruns of SMC. We can call this on any redist_plans object\nand it will adjust the output information depending on what algorithm\ngenerated the plans.\n\n\nsummary(plans)\n\n\n\n\nR-hat values for summary statistics:\n frac_kept dvs_gov_18 county_spl \n   1.00259    1.00197    0.99955 \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1       485 (97.0%)      9.9%        0.38   469 ( 94%)     10 \nSplit 2       474 (94.9%)      5.3%        0.48   397 ( 79%)      6 \nResample      411 (82.2%)       NA%        0.48   417 ( 83%)     NA \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1       482 (96.5%)     12.2%        0.41   471 ( 94%)      8 \nSplit 2       476 (95.3%)      6.7%        0.46   405 ( 81%)      5 \nResample      417 (83.4%)       NA%        0.45   419 ( 84%)     NA \n\nEach R-hat value is below 1.05, so we do not get any warnings. At a\nhigh level, this means that both runs of SMC are sampling from regions\ncomparable by these three summary statistics. That isn’t always the case\nthough. If you do get a warning, you should increase the number of\nsimulations or decrease the constraint strengths.\nWe next introduce the new constraint interface. To initialize a\nconstraint, we call redist_constr, which takes a\nredist_map input.\n\n\nconstr <- redist_constr(map = map_nm)\n\n\n\nWe can add any of the many constraints available with\n?constraints. There are many new constraints to people who\nhave only used redist_smc/redist_mergesplit or\nredist_flip before. Now all constraints are available to\nall algorithms. Additionally, we can write pretty much any constraint\nthat we can map to the positive reals, using the new custom\nconstraint.\nFor our custom constraint, we just care that the 100th row of\nmap_nm won’t be assigned to district 3. We can do the\nfollowing\n\n\nconstr <- constr %>% \n    add_constr_custom(\n        strength = 10,\n        fn = function(plan, distr) {\n            as.numeric(plan[100] != 3)\n        }\n    )\n\n\n\nThis takes an R function fn and a strength value (how\nmuch to multiply the output of fn by). The fn\ninput should always take the form\nfunction(plan, distr) { ... }, where plan will\nbe an integer matrix of precinct-district assignments and\ndistr will be the current district.\nWe can then pass constr to the constraints\nargument in redist_smc().\n\n\nset.seed(2022)\nplans <- redist_smc(map = map_nm, nsims = 500, runs = 2, counties = county, \n                    constraints = constr)\n\n\n\nAgain, we add some summary statistics.\n\n\nplans <- plans %>% \n    mutate(\n        frac_kept = comp_frac_kept(plans = ., shp = map_nm),\n        dvs_gov_18 = part_dvs(plans = ., shp = map_nm, dvote = gov_18_dem_luj, rvote = gov_18_rep_pea),\n        county_spl = splits_admin(plans = ., shp = map_nm, admin = county)\n    )\n\n\n\nThen run the diagnostics:\n\n\nsummary(plans)\n\n\n\n\nR-hat values for summary statistics:\n frac_kept dvs_gov_18 county_spl \n   0.99916    1.00000    1.00845 \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1       484 (96.8%)     10.9%        0.38   475 ( 95%)     10 \nSplit 2       476 (95.1%)      5.7%        0.46   399 ( 80%)      6 \nResample      413 (82.6%)       NA%        0.46   407 ( 81%)     NA \n\n         Eff. samples (%) Acc. rate Log wgt. sd  Max. unique Est. k \nSplit 1       483 (96.6%)     12.8%        0.40   478 ( 96%)      8 \nSplit 2       477 (95.4%)      7.3%        0.45   388 ( 78%)      5 \nResample      419 (83.8%)       NA%        0.45   420 ( 84%)     NA \n\nAnd everything looks good. Despite adding a constraint, the sample\nstill looks fine under these summary statistics.\nFor more information on diagnostics, take a look at McCartan and Imai\n(2022).\n\nFor a very brief intro to\nredist_maps, see the 3.0 release post at https://alarm-redist.github.io/posts/2021-04-02-redist-300/.↩︎\n",
    "preview": "posts/2022-06-20-redist-40/redist-40_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-04-23-47-prefecture-project/",
    "title": "47-Prefecture Project",
    "description": "Using redistricting simulation methods to better understand redistricting in Japan.",
    "author": [
      {
        "name": "Sho Miyazaki",
        "url": {}
      },
      {
        "name": "Kento Yamada",
        "url": {}
      },
      {
        "name": "Rei Yatsuhashi",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2022-04-23",
    "categories": [],
    "contents": "\nThe ALARM Project is pleased to announce the 47-Prefecture Project, whose goal is to generate and analyze redistricting plans for the single-member districts of the House of Representatives of Japan using redistricting simulation algorithms.\n\nLearn more about the project and see the results »\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-06-2021-10-06-das-published/",
    "title": "Revised and published: The use of differential privacy for census data and its impact on redistricting",
    "description": "A new postscript analyzes the final version of the U.S. Census Bureau's Disclosure Avoidance System.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nAt the end of May, the ALARM Project released a report examining “The Impact U.S. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis.” The Census Bureau has since updated the system parameters and released 2020 Census Data protected under this Disclosure Avoidance System.\nOur original report has been revised to include an analysis of the new system parameters, and appears today in the journal Science Advances.\nRead the paper: The use of differential privacy for census data and its impact on redistricting: The case of the 2020 U.S. Census\n\n\n\n\n",
    "preview": "https://pbs.twimg.com/media/FA8fplwXsAMVffB?format=jpg&name=large",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-census-2020/",
    "title": "2020 Redistricting Data Files",
    "description": "Census and election data joined together for use in redistricting\nand voting rights analysis.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\n\nAKALARAZCACOCTDEFLGAHIIAIDILINKSKYLAMAMDMEMIMNMOMSMTNCNDNENHNJNMNVNYOHOKORPARISCSDTNTXUTVAVTWAWIWVWYDCUSA\n\nThe ALARM Project is glad to provide precinct-level demographic and election data\nfrom the 2020 decennial census and the\nVoting and Election Science Team\nwhich have been tidied and joined together using 2020 precinct boundaries.\nWhere 2020 precinct boundaries are not available, Census block-level data is\nprovided instead, and where no VEST data is available, only demographic\ninformation is provided. Code to generate the data from these sources is\nincluded; the entire workflow is open-source and reproducible.\nGetting the data\nDownload individual states’ data below, or\ndownload a ZIP of all the data here.\nOur repository also contains\nmore detailed data, as well as code and instructions for programmatic\ndownloading, adding shapefile geometries, and other use cases.\nPlease make sure to cite the\nVoting and Election Science Team\nand the U.S. Census Bureau.\nConsult the license\nfor information on modifying and sharing the data and/or code.\n2020 state data\n\n\n\n\n \n\nAlabama\n \nVTDs\n\n \n\n \nal_2020_vtd.csv\n\n\n\n\n\n \n\nAlaska\n \nVTDs\n\n \n\n \nak_2020_vtd.csv\n\n\n\n\n\n \n\nArizona\n \nVTDs\n\n \n\n \naz_2020_vtd.csv\n\n\n\n\n\n \n\nArkansas\n \nVTDs\n\n \n\n \nar_2020_vtd.csv\n\n\n\n\n\n \n\nCalifornia\n \nCensus blocks\n\n \n\n \nca_2020_block.csv\n\n\n\n\n\n \n\nColorado\n \nVTDs\n\n \n\n \nco_2020_vtd.csv\n\n\n\n\n\n \n\nConnecticut\n \nVTDs\n\n \n\n \nct_2020_vtd.csv\n\n\n\n\n\n \n\nDelaware\n \nVTDs\n\n \n\n \nde_2020_vtd.csv\n\n\n\n\n\n \n\nDistrict of Columbia\n \nVTDs\n\n \n\n \ndc_2020_vtd.csv\n\n\n\n\n\n \n\nFlorida\n \nVTDs\n\n \n\n \nfl_2020_vtd.csv\n\n\n\n\n\n \n\nGeorgia\n \nVTDs\n\n \n\n \nga_2020_vtd.csv\n\n\n\n\n\n \n\nHawaii\n \nCensus blocks\n\n \n\n \nhi_2020_block.csv\n\n\n\n\n\n \n\nIdaho\n \nVTDs\n\n \n\n \nid_2020_vtd.csv\n\n\n\n\n\n \n\nIllinois\n \nVTDs\n\n \n\n \nil_2020_vtd.csv\n\n\n\n\n\n \n\nIndiana\n \nVTDs\n\n \n\n \nin_2020_vtd.csv\n\n\n\n\n\n \n\nIowa\n \nVTDs\n\n \n\n \nia_2020_vtd.csv\n\n\n\n\n\n \n\nKansas\n \nVTDs\n\n \n\n \nks_2020_vtd.csv\n\n\n\n\n\n \n\nKentucky\n \nVTDs\n\n \n\n \nky_2020_vtd.csv\n\n\n\n\n\n \n\nLouisiana\n \nVTDs\n\n \n\n \nla_2020_vtd.csv\n\n\n\n\n\n \n\nMaine\n \nVTDs\n\n \n\n \nme_2020_vtd.csv\n\n\n\n\n\n \n\nMaryland\n \nVTDs\n\n \n\n \nmd_2020_vtd.csv\n\n\n\n\n\n \n\nMassachusetts\n \nVTDs\n\n \n\n \nma_2020_vtd.csv\n\n\n\n\n\n \n\nMichigan\n \nVTDs\n\n \n\n \nmi_2020_vtd.csv\n\n\n\n\n\n \n\nMinnesota\n \nVTDs\n\n \n\n \nmn_2020_vtd.csv\n\n\n\n\n\n \n\nMississippi\n \nVTDs\n\n \n\n \nms_2020_vtd.csv\n\n\n\n\n\n \n\nMissouri\n \nVTDs\n\n \n\n \nmo_2020_vtd.csv\n\n\n\n\n\n \n\nMontana\n \nVTDs\n\n \n\n \nmt_2020_vtd.csv\n\n\n\n\n\n \n\nNebraska\n \nVTDs\n\n \n\n \nne_2020_vtd.csv\n\n\n\n\n\n \n\nNevada\n \nVTDs\n\n \n\n \nnv_2020_vtd.csv\n\n\n\n\n\n \n\nNew Hampshire\n \nVTDs\n\n \n\n \nnh_2020_vtd.csv\n\n\n\n\n\n \n\nNew Jersey\n \nVTDs\n\n \n\n \nnj_2020_vtd.csv\n\n\n\n\n\n \n\nNew Mexico\n \nVTDs\n\n \n\n \nnm_2020_vtd.csv\n\n\n\n\n\n \n\nNew York\n \nVTDs\n\n \n\n \nny_2020_vtd.csv\n\n\n\n\n\n \n\nNorth Carolina\n \nVTDs\n\n \n\n \nnc_2020_vtd.csv\n\n\n\n\n\n \n\nNorth Dakota\n \nVTDs\n\n \n\n \nnd_2020_vtd.csv\n\n\n\n\n\n \n\nOhio\n \nVTDs\n\n \n\n \noh_2020_vtd.csv\n\n\n\n\n\n \n\nOklahoma\n \nVTDs\n\n \n\n \nok_2020_vtd.csv\n\n\n\n\n\n \n\nOregon\n \nCensus blocks\n\n \n\n \nor_2020_block.csv\n\n\n\n\n\n \n\nPennsylvania\n \nVTDs\n\n \n\n \npa_2020_vtd.csv\n\n\n\n\n\n \n\nRhode Island\n \nVTDs\n\n \n\n \nri_2020_vtd.csv\n\n\n\n\n\n \n\nSouth Carolina\n \nVTDs\n\n \n\n \nsc_2020_vtd.csv\n\n\n\n\n\n \n\nSouth Dakota\n \nVTDs\n\n \n\n \nsd_2020_vtd.csv\n\n\n\n\n\n \n\nTennessee\n \nVTDs\n\n \n\n \ntn_2020_vtd.csv\n\n\n\n\n\n \n\nTexas\n \nVTDs\n\n \n\n \ntx_2020_vtd.csv\n\n\n\n\n\n \n\nUtah\n \nVTDs\n\n \n\n \nut_2020_vtd.csv\n\n\n\n\n\n \n\nVermont\n \nVTDs\n\n \n\n \nvt_2020_vtd.csv\n\n\n\n\n\n \n\nVirginia\n \nVTDs\n\n \n\n \nva_2020_vtd.csv\n\n\n\n\n\n \n\nWashington\n \nVTDs\n\n \n\n \nwa_2020_vtd.csv\n\n\n\n\n\n \n\nWest Virginia\n \nVTDs\n\n \n\n \nwv_2020_vtd.csv\n\n\n\n\n\n \n\nWisconsin\n \nVTDs\n\n \n\n \nwi_2020_vtd.csv\n\n\n\n\n\n \n\nWyoming\n \nVTDs\n\n \n\n \nwy_2020_vtd.csv\n\n\n\n\nUsing the data\nData Format\nEach data table contains several identification columns, a set of census-derived\ndemographic columns, and a set of VEST-derived election columns.\nGEOID20 is the unique identifier for a precinct or Census block.\nThe state and county of the precinct or block are also provided.\nCensus variables are prefixed with pop_ or vap_, depending on whether\nthey are for the entire population or the voting-age population.\nSuffixes refer to racial and ethnic categories, as follows:\n_hisp: Hispanic or Latino (of any race)\n_white: White alone, not Hispanic or Latino\n_black: Black or African American alone, not Hispanic or Latino\n_aian: American Indian and Alaska Native alone, not Hispanic or Latino\n_asian: Asian alone, not Hispanic or Latino\n_nhpi: Native Hawaiian and Other Pacific Islander alone, not Hispanic or Latino\n_other: Some Other Race alone, not Hispanic or Latino\n_two: Population of two or more races, not Hispanic or Latino\n\nElection variables consist of average vote counts for Democratic and\nRepublican candidates. The adv_## and arv_## columns report the\naverage vote count in the ## election, across all statewide races\ncontested by both parties. The ndv and nrv columns further average\nthe vote counts across all available election years. For specific statewide\nraces, you may download the files in vest-2020/ and join them to the data\nusing the GEOID20 column.\nMore Tools\nFor redistricting and voting rights analysis, we recommend the\nredist package.\nFor pre-processing and tidying data for redistricting analysis, we recommend the\ngeomander package.\nFor more custom tabulations of the 2020 census data, we recommend the\nPL94171 package.\nFor general-purpose census data processing, we recommend the\ncensable package.\nFor alternate data unaffected by Census differential privacy, you may want to\nconsider FCC block-level estimates, available using the\nblockpop package.\nTechnical notes\nTo produce election data using 2020 precinct boundaries, election results were\nprojected down to the 2010 block level using voting-age population as weights.\nResults for 2020 blocks were then estimated using 2010 blocks and the\nland-use-based crosswalk files\nfrom VEST. Finally, 2020 blocks were aggregated to 2020 precincts using the\nCensus’ 2020 block assignment files.\n2010 Data Addendum\nIf you are looking for a similar construction for 2010 data, please see here.\n\n\n\n",
    "preview": "https://alarm-redist.github.io/posts/2021-08-10-census-2020/graphic.png",
    "last_modified": "2023-05-23T08:57:47-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-05-revised-das-impact/",
    "title": "Revised: Impact of the Census Disclosure Avoidance System",
    "description": "We are releasing an updated version of our analysis of the U.S. Census'\nprivacy protection system and its impacts on the redistricting process.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-07-05",
    "categories": [],
    "contents": "\nLast month, the ALARM Project released a report examining “The Impact U.S. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis.” We thank everyone who provided us with feedback on this first version, which was written as a comment on the April 28, 2021 Demonstration Data. The Census Bureau has since updated the system parameters, and as such, we are releasing an updated version.\nRevised report: The Impact of the U.S. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis\nNotably, we expand our study of bias related to racial heterogeneity to include four new states of interest: Alabama, Delaware, Utah, and Washington. We also expand our studies of partisan and racial effects in redistricting to include more cases and provide additional information on the ranges of possible outcomes. These changes come alongside a restructuring of the paper and several smaller edits to recognize additional work focused on the new DAS methodologies used this year by the Census Bureau.With this version, we are also releasing replication data and code, available on the Harvard Dataverse.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-09-dsep-decision-response/",
    "title": "Reaction to the Census Bureau's Updated Parameters",
    "description": "The Data Stewardship Executive Policy Committee announces a higher privacy \nloss budget and other changes to the Disclosure Avoidance System.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-06-09",
    "categories": [],
    "contents": "\nOn June 9, The Census Bureau released information on their final parameters for the 2020 Census data release. We are grateful that they have incorporated some of the recommendations from our report to help build a better data product for redistricting.\nThe Bureau has made several welcome changes. They state that they have updated the post-processing component of the Disclosure Avoidance System (DAS) to address the undercounting bias in racially and ethnically diverse areas, which we reported in our analysis. As we recommended, the Bureau has also increased the privacy-loss budget, allocating the increase towards more accurate population and racial counts on geographies at the block group level and higher. We are hopeful that this targeted increase will help attenuate the DAS- induced population change at the voting district and precinct level.\nSome unresolved issues remain, however. The Bureau indicates that a new demonstration data product will not be released until September. Yet they plan to first release 2020 census redistricting data, which we expect states and localities to begin using immediately, by August 16, 2021.1 Given the timing, it is unclear how the evaluation of the new demonstration data will affect the upcoming redistricting process and related litigation.  The Bureau also states that there will be no direct increases in accuracy for block-level data, in order to protect privacy.  As this is a key aspect of ensuring the equality of vote and protecting the principle of One Person, One Vote, map drawers and analysts may have to adjust their interpretation of this long-standing principle.\nThe Bureau also announced that they plan to release the final version of the DAS code base.  This is an important step for transparency. However, the code alone does not allow analysts to evaluate the impacts of DAS on redistricting and properly account for the additional uncertainty due to the injected noise. We recommend the Bureau also release differentially private noisy population counts that have not been subject to the post-processing steps, as well as parameter values used for noise generation. Although scholarly communities have not fully resolved the issue of incorporating additional noise into redistricting simulation analysis, the availability of such information should facilitate future methodological development. In particular, it is of interest to examine whether or not the additional noise makes it more difficult to detect partisan and racial gerrymandering.\n\nCaliper, who makes the software Maptitude for Redistricting, has announced they will process the legacy format data, and two ALARM Project members have created an R package to process legacy data as well. These types of resources should allow people to immediately begin drawing maps.↩︎\n",
    "preview": "posts/2021-06-09-dsep-decision-response/census_slide.jpg",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-02-das-evaluation-faq/",
    "title": "FAQ: Impact of the Census Disclosure Avoidance System",
    "description": "Answers to common questions about our recently-released report \nevaluating the Census' Disclosure Avoidance System.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-06-02",
    "categories": [],
    "contents": "\n\nContents\nAre you advocating for the use of swapping methods over differential privacy as a method of privacy protection?\nDoes the DAS noise “cancel out” at larger geographic scales?\nWill the noisy data make it harder to create partisan gerrymanders?\nWhy is using a simulation-based method useful to assess DAS? What are the downsides?\nWhy do you use strict thresholds to define equal district population parity, thereby labelling more plans invalid?\nWhy do you use a strict threshold of 50% to define the majority-minority districts (MMDs)?\nWhy does your analysis not adjust for the differential privacy (DP) mechanism, to properly measure uncertainty?\nDo your findings about the prediction of individual race contradict the property of differential privacy?\nWhat are the differences between your analysis and MGGG’s analysis, both of which are based on simulation methods?\nWhy do you rely on Census data as the ‘ground-truth’? Doesn’t the Census itself already have enumeration error?\n\nWe have received a large number of questions and suggestions since releasing the first version of our report on Friday, May 28th. After meeting the Census Bureau’s one month deadline to produce comments on DAS-protected data, we are continuing to undertake additional analyses of the impacts of DAS on the redistricting process and analyses. We are working towards releasing the revised report soon. In the meantime, we provide our current answers to the questions frequently asked by others below.\nAre you advocating for the use of swapping methods over differential privacy as a method of privacy protection?\nNo. We have not studied the impact of swapping on redistricting. It is well known that differential privacy (DP) is theoretically superior to swapping in terms of privacy protection. The key policy question is how much privacy protection we would want at the expense of accurate census measurements. We argue that when changing an important public policy, such as whether additional noise should be added to the census, one needs to carefully determine the impacts of such policy change. We do not argue against DP and believe additional privacy protections to be a worthy consideration for some of the Census variables and products. However, we find that the current implementation of DAS can lead to substantively important changes in redistricting outcomes while also not meaningfully protecting privacy (as demonstrated by the fact that the prediction of individual race remains accurate even with the DAS data). We believe that more studies are needed to determine whether to inject noise and if so how exactly noise should be added. We hope that our study, along with many others, is the first step in answering this important question, by showing the potential consequences of the current DAS on certain redistricting outcomes.\nDoes the DAS noise “cancel out” at larger geographic scales?\nTheoretically yes, but with a caveat. The DAS is designed to have less noise at larger geographic scales, especially for levels of geography in the Census hierarchy, known as “on-spine” geographies (blocks, block groups, tracts, and counties). But geographies like Census places and voting precincts (VTDs) are “off-spine”. The design of the DAS may induce noise that does not cancel out in off-spine geographies.\nOur analysis finds that not only is there more noise for VTDs, but also that there remains a particular form of previously undiscussed bias — perhaps an unintentional side-effect of the DAS post-processing procedure needing to satisfy accuracy constraints in on-spine geographies. We find that the DAS data systematically undercounts racially and politically diverse VTDs in comparison to more homogeneous VTDs. How these discrepancies add up into legislative districts clearly depends on the spatial adjacency of diverse and homogenous VTDs. But in some cases the bias does not cancel out. In Pennsylvania, the average Congressional district changes by only 400 or so people. But the majority-Black 3rd Congressional District gains around 2,000 people under the DAS-protected data, while the more diverse 2nd Congressional District, a Black-Hispanic coalition district, loses around 2,000 people.\nWill the noisy data make it harder to create partisan gerrymanders?\nIn general, no. Partisan gerrymanders can be made with election data and voter files, both of which are not part of the Census and so will not contain any DAS-based error. Other residential and political data sources (both public and private) exist as well, which could be used to draw skewed districts. However, for analysts who must use noisy Census data to identify and analyze a potential gerrymander, the noise and the partisan biases discussed above may make it more difficult to do so.\nWhy is using a simulation-based method useful to assess DAS? What are the downsides?\nWithout the use of simulation, we are restricted to verifying changes in enacted redistricting plans. Analyzing enacted plans under a particular DAS dataset may not allow us to understand how DAS affects other similar plans. By using simulation methods, we are able to draw new maps with the DAS data under realistic constraints while taking into account the geography and spatial distributions of populations.  A downside of simulation is that the results will depend on the types of redistricting maps the algorithms are designed to generate.  More studies are needed to better understand how DAS affects different types of redistricting maps.\nWhy do you use strict thresholds to define equal district population parity, thereby labelling more plans invalid?\nThe use of strict thresholds is informed by a long series of Supreme Court decisions which establish and define the “one person, one vote” principle.  Current practice for redistricting congressional districts is to minimize the population difference across as much as possible.  As summarized by the National Conference of State Legislatures, in many states, this means getting district populations to be within a single person of one another, based on the Census counts.  If only noisy data is available, drawing districts to minimize the population difference becomes at best ambiguous.  Our analysis demonstrates the current statutory and judicial standards may not be applicable to the DAS-protected data.  On this point, our findings about the magnitude of population changes agree with many other analyses of the DAS and the April 28 Demonstration Data.  If the DAS data will be used for redistricting, the key question will be if and how to change the interpretation of “one person, one vote” principle and how such a change will affect redistricting in practice.\nWhy do you use a strict threshold of 50% to define the majority-minority districts (MMDs)?\nWe use a 50% threshold for majority minority districts to remain in line with the jurisprudence following Thornburg v. Gingles (1986). Specifically, Bartlett v. Strickland (2009) held that a minority must “constitute a numerical majority of the voting-age population in an area before §2 requires the creation of a legislative district to prevent dilution of that group’s votes.” When simulating districts, the number of districts which satisfy this specific numerical constraint is thus important. There are other ways to think about minority voting in elections, such as “opportunity districts”, “coalitional districts”, “crossover districts”, and “influence districts.” These have less concrete definitions and rely on electoral data, rather than just Census data.  Certainly, strict majorities are not the only relevant measure of minority voting power, and its coarseness makes it sensitive to change introduced by the DAS.  We find that the practice of using strict thresholds for majority minority districts may lead to biased results especially for small districts such as state legislative districts and school boards.\nWhy does your analysis not adjust for the differential privacy (DP) mechanism, to properly measure uncertainty?\nOur analysis examines the consequences of a map drawer taking DAS-protected data as-is. Ideally, analysts can also adjust for the DP mechanism into that data.  However, the Bureau’s deterministic and asymmetric post-processing procedure cannot be easily incorporated into standard redistricting analyses, preventing analysts from quantifying the added uncertainty exactly.  We recommend that the Bureau release the DP data without post-processing, allowing analysts to adjust redistricting plans with this added uncertainty. \nDo your findings about the prediction of individual race contradict the property of differential privacy?\nTechnically, no. Differential privacy guarantees that the inference based on a database does not depend on whether or not a particular individual is included in the data.  However, one might argue that for the PL94-171 data, the only sensitive information to be protected is race.  Our prediction methodology combines the census block level racial composition with the publicly available addresses and names of registered voters.  We found that our overall prediction performance does not degrade even if we use the noise-added census block data. And, yet, in our reanalysis of a recent court case where this prediction methodology was prominently used, the prediction appears to be substantively different.  We are in the process of studying when and how these differences arise.  In general, when deciding what privacy to protect at the cost of inaccurate measurements, it is important to consider the consequences of privacy disclosure.  Our finding suggests that the addition of noise to the census data does not necessarily improve the privacy protection in terms of individuals’ race.\nWhat are the differences between your analysis and MGGG’s analysis, both of which are based on simulation methods?\nThe MGGG Redistricting Lab has also released a study on using DAS data for redistricting purposes, which we believe is well done. In many ways, our results agree with those of the MGGG team. Like the MGGG study, we also find district-level population errors on the scale of hundreds or thousands of people depending on the size of the district (including Section 5 in our analysis and Figure 4 in the MGGG analysis). The core differences in these results come from interpretation. We find similarly sized errors and consider them to be major differences given that districts in many scenarios are drawn to be as exact as possible, down to individual people in many states. For example, Karcher v. Daggett (1983) found that even minute differences in population parity across congressional districts must be justified, even when smaller than the expected error in the decennial census. The MGGG team interprets their results in light of existing noise in current Census data, and say that “the practice of one-person population deviation across districts was never reasonably justified by the accuracy of Census data nor required by law.”  This implies that if DAS data are to be adopted, courts must decide how to change the interpretation of “one person, one vote” principle.\nOur analysis differs from the MGGG paper in several ways as well, and more studies are needed in order to better understand the impact of DAS on redistricting analysis and evaluation. First, our analysis relies on different data. We use the April 28th Demonstration Data, which the Census says will “closely approximate” the final data product, and span several states. The MGGG team uses the publicly available implementation of the 2018 TopDown algorithm with a reconstruction experiment to study Texas, mostly Dallas County, so our studies do not overlap in geographic areas.\nSecond, our analysis is focused on voting precincts (VTDs), which are the building blocks used in constructing redistricting plans in the vast majority of states.  Election results are also reported at this level or the state equivalent. In contrast, the MGGG analysis built districts out of Census blocks, block groups, and tracts.  These geographies are called “on-spine” by the Census bureau, since they nest within each other. Crucially, the DAS is designed to minimize error for these on-spine geographies, but not for off-spine geographies like VTDs. This may explain why our analysis finds more noise in the VTD population and racial counts than the MGGG’s analysis.\nWhy do you rely on Census data as the ‘ground-truth’? Doesn’t the Census itself already have enumeration error?\nCensus data, as acknowledged by the Bureau, has errors as well, including undercounts of some minority groups. Despite these inaccuracies, we rely on Census data for our simulations because governments and courts generally consider Census data as “ground-truth” for the purposes of drawing districts (e.g., Karcher v. Daggett 1983). The purpose of our analysis is to investigate how relying on DAS data, rather than on Census data as has generally been done in the past, may lead to different results in some cases. In an ideal world, districts drawn using DAS data would be substantively identical to those drawn with Census data. However, we find cases where districts differ by hundreds or thousands of people depending on whether one relies on the Census or DAS data. Additionally, we believe that more research is necessary to determine whether or not existing enumeration errors will cancel out the error induced by DAS, particularly for “off-spine” but substantively important geographies like VTDs.\nAs noted by other researchers, enumeration error is also substantively different from the random error injected as part of the DAS.  At small scales like Census blocks, DAS-injected noise is likely on a larger scale than enumeration error—some Census places have seen their population doubled or halved under the DAS, while such a result is unlikely with enumeration methods. As others have pointed out, this has the effect of adding a large random element on top of the more systematic enumeration errors, which tend to overcount or undercount larger areas or entire minority groups.  DAS does not remove or address these systematic errors; it only adds more error on top of them.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-28-census-das/",
    "title": "Impact of the Census Disclosure Avoidance System on Redistricting",
    "description": "In attempting to protect the privacy of 2020 Census respondents, the Census \nBureau has made its data unsuitable for redistricting purposes.",
    "author": [
      {
        "name": "Christopher T. Kenny",
        "url": {}
      },
      {
        "name": "Shiro Kuriwaki",
        "url": {}
      },
      {
        "name": "Cory McCartan",
        "url": {}
      },
      {
        "name": "Evan Rosenman",
        "url": {}
      },
      {
        "name": "Tyler Simko",
        "url": {}
      },
      {
        "name": "Kosuke Imai",
        "url": {}
      }
    ],
    "date": "2021-05-28",
    "categories": [],
    "contents": "\nThe U.S. Census Bureau plans to protect the privacy of 2020 Census respondents through its Disclosure Avoidance System (DAS), which attempts to achieve differential privacy guarantees by adding noise to the Census data. The Bureau has asked for feedback on the adequacy of DAS-protected data for real-world purposes. The ALARM project has conducted an extensive analysis into how the DAS-protected data affects redistricting and voting rights analyses, and has submitted these findings to the Census Bureau.\nRead the report: The Impact of the U.S. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis\nAlso see: Answers to Frequently Asked Questions Added on June 2, 2021.\nA figure from the report, indicating partisan biases in the DAS-protected data.By applying redistricting simulation and analysis methods to DAS-protected 2010 Census data, we find that the protected data are not of sufficient quality for redistricting purposes. Compared to the original Census 2010 data, we find that the DAS-protected data:\nPrevent map drawers from creating districts that satisfy the One Person, One Vote principle, according to current statutory and judicial standards. Actual deviations from equal population will generally be several times larger than as reported under the DAS data. The magnitude of this problem increases for smaller districts such as state legislative districts and school boards.\nTransfer population from low-turnout, mixed-party areas to high-turnout, single-party areas. This differential bias leads to different district boundaries, which in turn implies significant and unpredictable differences in election results. The discrepancy also degrades the ability of analysts to reliably identify partisan gerrymanders.\nTransfer population from racially mixed areas to racially segregated areas. This bias effectively means racially heterogeneous areas are under-counted. The degree of racial segregation can therefore be over-estimated, which can lead to a change in the number of majority-minority districts. It also creates significant precinct-level variability, which adds substantial unpredictability to whether or not a minority voter is included in a majority-minority district.\nAlter individual-level race predictions constructed from voter names and addresses. This leads to fewer estimated minority voters and majority-minority districts in a re-analysis of a recent Voting Rights Act case, NAACP v. East Ramapo School District. At a statewide level, however, the DAS data does not curb the ability of algorithms to identify the race of voters from names and addresses. Therefore, this casts doubt on the universal privacy protection guarantee of DAS data.\nOur primary recommendation is to release Census P.L. 94-171 data without using the Disclosure Avoidance System, and instead rely on a swapping method similar to that applied to the 2010 Census data in order to protect respondent privacy.\nIf the Census Bureau decides to apply the current DAS to Census PL. 94-171 Data, we recommend increasing the privacy loss budget and allocating the increase to improving redistricting outcomes. In particular, preserving the accuracy of populations at the voting tabulation district level would be critical. The Bureau must avoid injecting noise that systematically undercounts certain racial and partisan groups in the privacy-protected data. Additional recommendations are given in the paper.\n\n\n\n",
    "preview": "posts/2021-05-28-census-das/ex_plot.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 1200
  },
  {
    "path": "posts/2021-04-02-redist-300/",
    "title": "redist 3.0",
    "description": "A major release brings new algorithms, new workflows, and significant \nusability improvements.",
    "author": [
      {
        "name": "Cory McCartan",
        "url": "https://corymccartan.github.io/"
      },
      {
        "name": "Christopher Kenny",
        "url": "https://www.christophertkenny.com/"
      }
    ],
    "date": "2021-04-07",
    "categories": [],
    "contents": "\nThe ALARM Project is excited to announce the release of redist 3.0 on CRAN. This release brings with it new algorithms and major new workflow improvements, making redistricting analysis broadly accessible to data scientists everywhere.\n\nInstall the new version with install.packages(\"redist\").\nNew Features\nThis release includes far too many changes to list comprehensively. Key improvements and new features include:\nNew tidy interface, including new redist_map and redist_plans objects\nMerge-split MCMC now available in redist_mergesplit()\nShort burst MCMC optimization now available in redist_shortburst() along with scoring functions\nImproved Flip MCMC interface and performance improvements\nNew support for larger simulation size limits\nFunctions to freeze parts of a map and extract district cores\nNew VRA constraint\nMany new plotting functions\nConsistent function and argument names\nNew partisanship and compactnes metrics\nPerformance improvements to compactness calculations\nPlan comparison and classification in compare_plans() and classify_plans()\nNew iowa dataset and cleaned-up package data\nTo begin exploring the new features, check out the new Get Started vignette.\nWorkflow Example: North Carolina\nTo demonstrate the new redist workflow, we’ll run through a basic analysis of the 2017 congressional districts of the state of North Carolina, which were struck down as an unconstitutional partisan gerrymander in 2019.\nNew workflow\n\n\nlibrary(tidyverse)\nlibrary(redist)\n\ndownload.file(\"https://github.com/alarm-redist/redist-data/raw/main/data/nc.rds\",\n              data_path <- tempfile())\nnc_shp <- readRDS(data_path) %>%\n    select(vtd:vap, el14g_uss_r:geometry)\n\n\n\nUnder the new workflow, a redistricting analysis begins with a redist_map object, which defines the basic parameters of the redistricting problem. The redist_map() constructor builds the precinct adjacency graph which is required for redistricting simulation, and stores relevant metadata, such as the desired population parity tolerance and a reference to the existing districts. It also comes with helpful plotting functions.\n\n\nnc = redist_map(nc_shp, existing_plan=cd_17, pop_tol=0.01)\nprint(nc)\n\n\nA redist_map object with 2692 units and 15 fields\nTo be partitioned into 13 districts with population between 733,499 - 1.0% and 733,499 + 1.0%\nWith geometry:\n    bbox:           xmin: 406820 ymin: 2696.2 xmax: 3070200 ymax: 1043600\n    projected CRS:  NAD83(HARN) / North Carolina (ftUS)\n# A tibble: 2,692 x 15\n   vtd       county   pop   vap el14g_uss_r el14g_uss_d el14g_uss_l\n * <chr>     <chr>  <int> <int>       <int>       <int>       <int>\n 1 3700106W  37001   1973  1505         181         182          17\n 2 3700112E  37001   3391  2503         180         271          21\n 3 3700112W  37001   2744  2156         457         481          42\n 4 3700106N  37001   4468  3167         231         466          31\n 5 37001126  37001   2038  1713         670         416          38\n 6 37001124  37001   2455  1948         491         391          33\n 7 370011210 37001   2802  2127         358         309          31\n 8 3700103N  37001   5712  4955        1063         853          53\n 9 3700102   37001   4491  3483        1246         313          62\n10 3700106E  37001   3113  2371         423         432          42\n# … with 2,682 more rows, and 8 more variables: el14g_uss_wi <int>,\n#   el14g_uss_tot <int>, cd_13 <int>, cd_17 <int>, aland10 <dbl>,\n#   awater10 <dbl>, geometry <MULTIPOLYGON [US_survey_foot]>,\n#   adj <list>\n\nplot(nc, el14g_uss_d/(el14g_uss_d+el14g_uss_r)) +\n    scale_fill_gradient2(midpoint=0.5)\n\n\n\n\nOnce we’ve created a redist_map object, we can simulate redistricting plans.\n\n\nplans = redist_smc(nc, 1000, counties=county, silent=TRUE) # 1000 plans\nprint(plans)\n\n\n1000 sampled plans and 1 reference plan with 13 districts from a 2692-unit map,\n  drawn using Sequential Monte Carlo\nWith plans resampled from weights\nPlans matrix: num [1:2692, 1:1001] 6 6 6 6 6 6 6 6 6 6 ...\n# A tibble: 13,013 x 3\n   draw  district total_pop\n   <fct>    <int>     <dbl>\n 1 cd_17        1    733323\n 2 cd_17        2    734740\n 3 cd_17        3    732627\n 4 cd_17        4    733218\n 5 cd_17        5    733879\n 6 cd_17        6    733554\n 7 cd_17        7    734750\n 8 cd_17        8    734777\n 9 cd_17        9    731507\n10 cd_17       10    736057\n# … with 13,003 more rows\n\nThe plans variable is a redist_plans object—a special container designed to make handling sets of redistricting plans painless. As the output above shows, plans contains the 1,000 samppled plans, plus the 2017 congressional districts. We can plot a few of these plans.\n\n\nredist.plot.plans(plans, draws=c(\"cd_17\", \"1\", \"2\", \"3\"), geom=nc)\n\n\n\n\nA redist_plans object makes it easy to compute plan and district summary statistics.\n\n\nplans = plans %>%\n    mutate(comp = distr_compactness(nc),\n           dem_share = group_frac(nc, el14g_uss_d, el14g_uss_d + el14g_uss_r))\nprint(plans)\n\n\n1000 sampled plans and 1 reference plan with 13 districts from a 2692-unit map,\n  drawn using Sequential Monte Carlo\nWith plans resampled from weights\nPlans matrix: num [1:2692, 1:1001] 6 6 6 6 6 6 6 6 6 6 ...\n# A tibble: 13,013 x 5\n   draw  district total_pop  comp dem_share\n   <fct>    <int>     <dbl> <dbl>     <dbl>\n 1 cd_17        1    733323 0.803     0.687\n 2 cd_17        2    734740 0.803     0.443\n 3 cd_17        3    732627 0.803     0.407\n 4 cd_17        4    733218 0.803     0.674\n 5 cd_17        5    733879 0.803     0.425\n 6 cd_17        6    733554 0.803     0.441\n 7 cd_17        7    734750 0.803     0.446\n 8 cd_17        8    734777 0.803     0.439\n 9 cd_17        9    731507 0.803     0.440\n10 cd_17       10    736057 0.803     0.419\n# … with 13,003 more rows\n\nFrom there, we can quickly generate informative plots. First we check the compactness of the generated plans, and see that they are significantly more compact than the adopted 2017 plan.\n\n\nhist(plans, comp) +\n    labs(x=\"Compactness score (higher is more compact)\")\n\n\n\n\nNext, we look at the partisan implications of the 2017 plan. We plot the two-party Democratic vote share in each district, with districts sorted by this quantity. Each dot on the plot below is a district from one simulated plan, and the red lines show the values for the 2017 plan.\n\n\nredist.plot.distr_qtys(plans, dem_share, size=0.1)\n\n\n\n\nWe see immediately that the 2017 plan packs Democratic voters into the three most Democratic districts, and cracks them in the remaining 10 districts, leading to a durable 10–3 Republican-Democratic seat split (in an election which Democrats captured 49% of the statewide two-party vote). A clear partisan gerrymander.\nStudying districts 1, 2, and 4\nIf we want to study a specific set of districts, we can quickly filter() to the relevant map area and re-run the analysis. The redist_map() object will handle all appropriate adjustments to the adjacency graph, number of districts, and population tolerance (as is visible below).\n\n\nnc_sub = filter(nc, cd_17 %in% c(1, 2, 4))\nprint(nc_sub)\n\n\nA redist_map object with 571 units and 15 fields\nTo be partitioned into 3 districts with population between 733,760 - 1.0353% and 733,760 + 0.96399%\nWith geometry:\n    bbox:           xmin: 1921600 ymin: 524880 xmax: 2784100 ymax: 1028200\n    projected CRS:  NAD83(HARN) / North Carolina (ftUS)\n# A tibble: 571 x 15\n   vtd     county   pop   vap el14g_uss_r el14g_uss_d el14g_uss_l\n * <chr>   <chr>  <int> <int>       <int>       <int>       <int>\n 1 37015C2 37015   2182  1707         174         503          13\n 2 37015M1 37015   1103   849         172         167           5\n 3 37015C1 37015   1229   986         229         184          11\n 4 37015MH 37015    992   811         146         254          12\n 5 37015W2 37015    966   764         286          47          20\n 6 37015W1 37015   7005  5703         596        1190          44\n 7 37015M2 37015   1290   983          99         239          16\n 8 37015SN 37015   1410  1025          63         327          11\n 9 37015WH 37015   1554  1274         292         262          12\n10 37015WD 37015   1409  1050          35         319           5\n# … with 561 more rows, and 8 more variables: el14g_uss_wi <int>,\n#   el14g_uss_tot <int>, cd_13 <int>, cd_17 <int>, aland10 <dbl>,\n#   awater10 <dbl>, geometry <MULTIPOLYGON [US_survey_foot]>,\n#   adj <list>\n\nplot(nc_sub)\n\n\n\n\nOn this subset, too, the adopted 2017 plan is a significant outlier.\n\n\nplans_sub = redist_smc(nc_sub, 1000, counties=county, silent=T) %>%\n    mutate(dem_share = group_frac(nc_sub, el14g_uss_d, el14g_uss_d + el14g_uss_r))\nredist.plot.distr_qtys(plans_sub, dem_share, size=0.3)\n\n\n\n\nOld workflow\nIn comparison, the old workflow required significantly more steps and manual processing.\n\n\nlibrary(tidyverse)\nlibrary(redist)\n\ndownload.file(\"https://github.com/alarm-redist/redist-data/raw/main/data/nc.rds\",\n              data_path <- tempfile())\nnc_shp <- readRDS(data_path) %>%\n    select(vtd:vap, el14g_uss_r:geometry)\n\n\n\nOnce we’ve downloaded the data, we can start by building the adjacency graph.\n\n\nadj <- redist.adjacency(nc_shp)\n\n\n\nTime to first simulation was never really the issue, however each simulation required many inputs. redist_map objects keep track of the adj, total_pop, ndists, and pop_tol arguments, but in the older version, you had to specify each of these for every simulation. One of the quirky aspects of the older version was that counties needed to be a vector with values 1:n_counties, meaning that you had to manually transform it to use it and that only worked if the counties were contiguous.\n\n\nsims <- redist.smc(adj = adj, total_pop = nc_shp$pop, ndists = 13, \n                   pop_tol = 0.01, \n                   counties = match(nc_shp$county, unique(nc_shp$county)), \n                   nsims = 1000, silent = TRUE)\n\n\n\nOnce you finished simulating, setting up plots was always a hassle, as you needed to plot both the distribution of simulations and then compute the same metric separately for the reference plan, in this case that’s the 2017 congressional districts.\n\n\nmetrics <- redist.metrics(plans = sims$plans, measure = 'DVS',\n                          rvote = nc_shp$el14g_uss_r, nc_shp$el14g_uss_d)\n\nsorted <- metrics %>% \n  group_by(draw) %>% \n  arrange(DVS,.by_group = TRUE) %>% \n  mutate(district = 1:13) %>% \n  ungroup()\n\nreference_metrics <- redist.metrics(plans = nc_shp$cd_17, \n                                    measure = 'DVS', \n                                    rvote = nc_shp$el14g_uss_r, \n                                    dvote = nc_shp$el14g_uss_d)\n\nsorted_reference <- reference_metrics %>% \n    arrange(DVS) %>% \n    mutate(district = 1:13)\n\n\n\nAnd then to plot the standard stacked boxplots, you would need to add the reference plan manually to the rest.\n\n\nsorted %>% ggplot(aes(x = district, y = DVS, group = district)) + \n  geom_boxplot() + \n  theme_minimal() + \n  labs(x = 'District, sorted by DVS') + \n  geom_segment(data = sorted_reference, size = 1,\n               aes(x = district - 0.35, xend = district + 0.35, \n                   yend = DVS, color = 'red')) + \n  scale_color_manual(name = '', values = c('red' = 'red'),\n  labels = c('ref'), guide = 'legend')\n\n\n\n\nStudying districts 1, 2, and 4\nThe steps between loading in data to your first simulation wasn’t terrible in the old version when you were working with the full map. However, when trying to work with subsets, it became messy.\nFirst you needed to subset the shape and then rebuild a new adjacency graph that only had the remaining precincts.\n\n\nsub <- nc_shp %>% filter(cd_17 %in% c(1, 2, 4))\nsub_adj <- redist.adjacency(sub)\n\n\n\nThen, if your target on the full map was 1%, you had to compute the equivalent on the subset map, as a 1% population deviation on a subset is often larger once recombined with the full map.\n\n\npop_tol <- 0.01\nsubparpop <- sum(sub$pop)/3\nparpop <- sum(nc_shp$pop)/13\n\nsub_pop_tol <-  min(abs(subparpop - parpop * (1 - pop_tol)),\n                abs(subparpop - parpop * (1 + pop_tol))) / subparpop\nsub_pop_tol\n\n\n[1] 0.0096399\n\nNow we can simulate again, but on the smaller map.\n\n\nsims_sub <- redist.smc(adj = sub_adj, total_pop = sub$pop,\n                      nsims = 1000,  ndists = 3, \n                      counties = match(sub$county, unique(sub$county)),\n                      pop_tol = sub_pop_tol, silent = TRUE)\n\n\n\nAs before, we have to compute metrics for both the reference plan and the simulated plans.\n\n\nsub_metrics <- redist.metrics(plans = sims_sub$plans, measure = 'DVS', \n                              rvote = sub$el14g_uss_r, sub$el14g_uss_d)\n\nsub_sorted <- sub_metrics %>% \n  group_by(draw) %>% \n  arrange(DVS,.by_group = TRUE) %>% \n  mutate(district = 1:3) %>% \n  ungroup()\n\nsub_reference_metrics <- redist.metrics(plans = match(sub$cd_17, \n                                                      unique(sub$cd_17)), \n                                        measure = 'DVS', \n                                        rvote = sub$el14g_uss_r, \n                                        dvote = sub$el14g_uss_d)\n\nsub_sorted_reference <- sub_reference_metrics %>%\n    arrange(DVS) %>% \n    mutate(district = 1:3)\n\n\n\nAnd finally, we can plot the metrics and manually add the reference points.\n\n\nsub_sorted %>% ggplot(aes(x = district, y = DVS, group = district)) + \n  geom_boxplot() + \n  theme_minimal() + \n  labs(x = 'District, sorted by DVS') + \n  geom_segment(data = sub_sorted_reference, size = 1,\n               aes(x = district - 0.35, xend = district + 0.35, \n                   yend = DVS, color = 'red')) + \n  scale_color_manual(name = '', values = c('red' = 'red'),\n  labels = c('ref'), guide = 'legend')\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-02-redist-300/redist-300_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-04-14T19:52:35-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
